{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorial Word Representations\n",
    "\n",
    "## Background\n",
    "Representing words as dense vectors over a finite-dimensional space was one of the recent breakthroughs in Natural Language Processing. Vectorial representations allow space-efficient, informationally rich storage of words that adequately captures their semantic content and enables numerical computation on them. Word vectors are the standard input representation for machine learning architectures for language processing. Even though new methods for constructing such representations emerge frequently, the original set of published papers remain a de facto point of reference as well as a good starting point. For this assignment, you will be asked to implement a small-scale variant of one such paper, namely [Global Word Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) (\"the GloVe paper\").\n",
    "\n",
    "Much of the code and data pre-processing has already been done for you. Additionally, notes on the paper will appear throughout the notebook to guide you along the code. It is, however, important to read and understand the paper, its terminology and the theory behind it before attempting to go through with the assignment. Some of the tasks will also require addressing the paper directly.\n",
    "\n",
    "**-------------------------------------------------------------------------------------------------------------**\n",
    "\n",
    "There are 2 types of tasks in this assignment: \n",
    "- coding tasks --- 10 tasks worth 1 point each --- asking you to write code following specifications provided; make sure to <ins>show the code to your teacher after completing every coding task</ins>\n",
    "- interpretation questions --- 5 questions worth 1 point each --- asking you to interpret the data or the results of the model\n",
    "\n",
    "You are greatly encouraged to add comments to your code describing what particular lines of code do (in general, a great habit to have in your coding life), as well as self-check regularly by printing your tensors and their shapes making sure they look adequate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics\n",
    "\n",
    "The paper's proposed model, GloVe, aims to densely represent words in a way that captures the global corpus statistics. \n",
    "\n",
    "The construction it encodes is the word __co-occurrence matrix__. A co-occurrence matrix is a simplistic data structure that counts the number of times each word has appeared within the context of every other word. The definition of a context varies; usually, context is implied to be a fixed-length span (that may or may not be allowed to escape sentence boundaries) around a word. \n",
    "\n",
    "For instance, in the sentence below and for a context length of 2, the word <span style=\"color:pink\">__Earth__</span> occurs in the context of <span style=\"color:lightgreen\">made</span> (1), <span style=\"color:lightgreen\">on</span> (1), <span style=\"color:lightgreen\">as</span> (1), <span style=\"color:lightgreen\">an</span> (1).\n",
    "\n",
    "> \"He struck most of the friends he had <span style=\"color:lightgreen\">made on</span> <span style=\"color:pink\">__Earth__</span> <span style=\"color:lightgreen\">as an</span> eccentric\"\n",
    "\n",
    "Similarly, the word <span style=\"color:pink\">__friends__</span> occurs in the context of <span style=\"color:lightgreen\">of</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">he</span> (1), <span style=\"color:lightgreen\">had</span> (1).\n",
    "\n",
    "> \"He struck most <span style=\"color:lightgreen\">of the</span> <span style=\"color:pink\">__friends__</span> <span style=\"color:lightgreen\">he had</span> made on Earth as an eccentric\"\n",
    "\n",
    "An alternative definition of a context would be, for instance, the variable-length windows spanned by a full sentence.\n",
    "\n",
    "Contexts may be summed across sentences or entire corpora; the summed context of <span style=\"color:pink\">he</span> in the example sentence is: <span style=\"color:lightgreen\">struck</span> (1), <span style=\"color:lightgreen\">most</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">friends</span> (1), <span style=\"color:lightgreen\">had</span> (1), <span style=\"color:lightgreen\">made</span> (1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this assignment, we have prepared a co-occurrence matrix over a minimally processed version of the Harry Potter books.\n",
    "\n",
    "(A few interpretation tasks in this assignment presuppose some minimal level of familiarity with the Harry Potter books/films. If no one in your group is familiar with Harry Potter, please talk to your teacher)\n",
    "\n",
    "The pickle file contains three items:\n",
    "1. `vocab`: a dictionary mapping words to unique ids, containing $N$ unique words\n",
    "2. `contexts`: a dictionary mapping words to their contexts, where contexts are themselves dicts from words to integers that show the number of co-occurrences between these words.\n",
    "    E.g. `{\"portrait\": {\"harry\": 103, \"said\": 97, ...}, ...}` meaning that the word \"harry\" has appeared in the context of the word \"portrait\" 103 times, etc.\n",
    "3. `X`: a torch LongTensor ${X}$ of size $N \\times N$, where ${X}[i,j]$ denotes the number of times the word with id $j$ has appeared in the context of the word with id $i$\n",
    "\n",
    "Extremely common or uncommon words (i.e. words with too few or too many global occurrences) have been filtered out for practical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch import FloatTensor, LongTensor\n",
    "from typing import Dict, Callable, List\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flavor', 'party', 'mistress', 'new', 'weird', 'pots', 'prodding', 'apparated', 'pressing', 'alastor']\n",
      "<class 'collections.Counter'>\n",
      "<class 'dict_items'>\n"
     ]
    }
   ],
   "source": [
    "with open(\"output.p\", \"rb\") as f:\n",
    "    vocab, contexts, X = pickle.load(f) \n",
    "    \n",
    "# Notes for self\n",
    "# ten words (will only print the key; not the value (id in this case))\n",
    "print(list(vocab)[:10])\n",
    "print(type(contexts['flavor']))\n",
    "print(type(contexts['flavor'].items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the summed context of the word 'portrait'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('harry', 103),\n",
       " ('said', 97),\n",
       " ('hole', 84),\n",
       " ('ron', 49),\n",
       " ('hermione', 46),\n",
       " ('room', 41),\n",
       " ('t', 40),\n",
       " ('fat', 39),\n",
       " ('lady', 39),\n",
       " ('common', 30),\n",
       " ('dumbledore', 26),\n",
       " ('phineas', 24),\n",
       " ('climbed', 22),\n",
       " ('just', 19),\n",
       " ('swung', 17),\n",
       " ('sirius', 17),\n",
       " ('professor', 17),\n",
       " ('time', 16),\n",
       " ('voice', 15),\n",
       " ('open', 15),\n",
       " ('got', 15),\n",
       " ('nigellus', 15),\n",
       " ('reached', 14),\n",
       " ('like', 14),\n",
       " ('came', 14),\n",
       " ('little', 14),\n",
       " ('gryffindor', 13),\n",
       " ('turned', 13),\n",
       " ('forward', 12),\n",
       " ('don', 12),\n",
       " ('long', 12),\n",
       " ('place', 12),\n",
       " ('wall', 11),\n",
       " ('neville', 11),\n",
       " ('black', 11),\n",
       " ('going', 11),\n",
       " ('snape', 11),\n",
       " ('hall', 11),\n",
       " ('mcgonagall', 11),\n",
       " ('corridor', 10),\n",
       " ('walked', 10),\n",
       " ('away', 10),\n",
       " ('ve', 10),\n",
       " ('way', 10),\n",
       " ('visit', 10),\n",
       " ('good', 10),\n",
       " ('did', 10),\n",
       " ('look', 10),\n",
       " ('password', 9),\n",
       " ('moment', 9),\n",
       " ('know', 9),\n",
       " ('sir', 9),\n",
       " ('opened', 9),\n",
       " ('face', 9),\n",
       " ('heard', 9),\n",
       " ('come', 8),\n",
       " ('gone', 8),\n",
       " ('asked', 8),\n",
       " ('let', 8),\n",
       " ('really', 8),\n",
       " ('entrance', 8),\n",
       " ('cadogan', 8),\n",
       " ('mother', 8),\n",
       " ('door', 8),\n",
       " ('pushed', 7),\n",
       " ('inside', 7),\n",
       " ('cloak', 7),\n",
       " ('looked', 7),\n",
       " ('picture', 7),\n",
       " ('staircase', 7),\n",
       " ('oh', 7),\n",
       " ('tower', 7),\n",
       " ('head', 7),\n",
       " ('ginny', 7),\n",
       " ('thought', 7),\n",
       " ('passed', 7),\n",
       " ('minister', 7),\n",
       " ('fudge', 7),\n",
       " ('scrambled', 6),\n",
       " ('looking', 6),\n",
       " ('bed', 6),\n",
       " ('hagrid', 6),\n",
       " ('himself', 6),\n",
       " ('point', 6),\n",
       " ('house', 6),\n",
       " ('castle', 6),\n",
       " ('tried', 6),\n",
       " ('coming', 6),\n",
       " ('need', 6),\n",
       " ('hurried', 6),\n",
       " ('stood', 6),\n",
       " ('saw', 6),\n",
       " ('people', 6),\n",
       " ('knew', 6),\n",
       " ('frame', 6),\n",
       " ('eyes', 6),\n",
       " ('dean', 6),\n",
       " ('wizard', 6),\n",
       " ('grimmauld', 6),\n",
       " ('headmaster', 6),\n",
       " ('won', 5),\n",
       " ('dress', 5),\n",
       " ('followed', 5),\n",
       " ('running', 5),\n",
       " ('seventh', 5),\n",
       " ('floor', 5),\n",
       " ('thing', 5),\n",
       " ('dormitory', 5),\n",
       " ('stairs', 5),\n",
       " ('pulled', 5),\n",
       " ('went', 5),\n",
       " ('m', 5),\n",
       " ('waiting', 5),\n",
       " ('yelled', 5),\n",
       " ('colin', 5),\n",
       " ('took', 5),\n",
       " ('left', 5),\n",
       " ('gryffindors', 5),\n",
       " ('talking', 5),\n",
       " ('doors', 5),\n",
       " ('felt', 5),\n",
       " ('office', 5),\n",
       " ('fred', 5),\n",
       " ('great', 5),\n",
       " ('set', 5),\n",
       " ('gave', 5),\n",
       " ('approached', 5),\n",
       " ('downstairs', 5),\n",
       " ('parchment', 5),\n",
       " ('dinner', 5),\n",
       " ('silver', 5),\n",
       " ('walk', 5),\n",
       " ('prime', 5),\n",
       " ('end', 4),\n",
       " ('hung', 4),\n",
       " ('woman', 4),\n",
       " ('pink', 4),\n",
       " ('percy', 4),\n",
       " ('reveal', 4),\n",
       " ('chair', 4),\n",
       " ('couldn', 4),\n",
       " ('care', 4),\n",
       " ('didn', 4),\n",
       " ('herself', 4),\n",
       " ('painting', 4),\n",
       " ('mind', 4),\n",
       " ('entered', 4),\n",
       " ('crept', 4),\n",
       " ('appeared', 4),\n",
       " ('stand', 4),\n",
       " ('short', 4),\n",
       " ('crowd', 4),\n",
       " ('spiral', 4),\n",
       " ('deserted', 4),\n",
       " ('caught', 4),\n",
       " ('began', 4),\n",
       " ('waited', 4),\n",
       " ('climbing', 4),\n",
       " ('large', 4),\n",
       " ('called', 4),\n",
       " ('boys', 4),\n",
       " ('past', 4),\n",
       " ('headed', 4),\n",
       " ('nearly', 4),\n",
       " ('years', 4),\n",
       " ('minutes', 4),\n",
       " ('closed', 4),\n",
       " ('later', 4),\n",
       " ('canvas', 4),\n",
       " ('able', 4),\n",
       " ('taken', 4),\n",
       " ('half', 4),\n",
       " ('simply', 4),\n",
       " ('ask', 4),\n",
       " ('sitting', 4),\n",
       " ('returned', 4),\n",
       " ('george', 4),\n",
       " ('night', 4),\n",
       " ('think', 4),\n",
       " ('lavender', 4),\n",
       " ('tell', 4),\n",
       " ('screaming', 4),\n",
       " ('old', 4),\n",
       " ('doing', 4),\n",
       " ('curtains', 4),\n",
       " ('figures', 4),\n",
       " ('ran', 4),\n",
       " ('odd', 4),\n",
       " ('painted', 4),\n",
       " ('bedroom', 4),\n",
       " ('desk', 4),\n",
       " ('barely', 4),\n",
       " ('silence', 4),\n",
       " ('explanation', 4),\n",
       " ('ugly', 4),\n",
       " ('man', 4),\n",
       " ('bag', 4),\n",
       " ('listen', 3),\n",
       " ('silk', 3),\n",
       " ('needed', 3),\n",
       " ('leg', 3),\n",
       " ('fireplace', 3),\n",
       " ('armchairs', 3),\n",
       " ('spoke', 3),\n",
       " ('believe', 3),\n",
       " ('wanted', 3),\n",
       " ('stop', 3),\n",
       " ('pig', 3),\n",
       " ('snout', 3),\n",
       " ('save', 3),\n",
       " ('packed', 3),\n",
       " ('use', 3),\n",
       " ('stopped', 3),\n",
       " ('managed', 3),\n",
       " ('climb', 3),\n",
       " ('invisibility', 3),\n",
       " ('sorry', 3),\n",
       " ('hurrying', 3),\n",
       " ('fight', 3),\n",
       " ('new', 3),\n",
       " ('impatiently', 3),\n",
       " ('standing', 3),\n",
       " ('leaving', 3),\n",
       " ('creevey', 3),\n",
       " ('lockhart', 3),\n",
       " ('trolls', 3),\n",
       " ('pointing', 3),\n",
       " ('wait', 3),\n",
       " ('gray', 3),\n",
       " ('immediately', 3),\n",
       " ('closing', 3),\n",
       " ('corridors', 3),\n",
       " ('hidden', 3),\n",
       " ('trouble', 3),\n",
       " ('threw', 3),\n",
       " ('right', 3),\n",
       " ('watch', 3),\n",
       " ('d', 3),\n",
       " ('rest', 3),\n",
       " ('students', 3),\n",
       " ('heads', 3),\n",
       " ('vanished', 3),\n",
       " ('crookshanks', 3),\n",
       " ('shut', 3),\n",
       " ('dormitories', 3),\n",
       " ('completely', 3),\n",
       " ('mad', 3),\n",
       " ('shall', 3),\n",
       " ('outside', 3),\n",
       " ('read', 3),\n",
       " ('tiny', 3),\n",
       " ('walls', 3),\n",
       " ('extremely', 3),\n",
       " ('met', 3),\n",
       " ('table', 3),\n",
       " ('grounds', 3),\n",
       " ('corner', 3),\n",
       " ('sight', 3),\n",
       " ('told', 3),\n",
       " ('circular', 3),\n",
       " ('hand', 3),\n",
       " ('silent', 3),\n",
       " ('noise', 3),\n",
       " ('eye', 3),\n",
       " ('admit', 3),\n",
       " ('stay', 3),\n",
       " ('clambered', 3),\n",
       " ('realized', 3),\n",
       " ('mrs', 3),\n",
       " ('taking', 3),\n",
       " ('expression', 3),\n",
       " ('movement', 3),\n",
       " ('hair', 3),\n",
       " ('slightly', 3),\n",
       " ('wearing', 3),\n",
       " ('wand', 3),\n",
       " ('trying', 3),\n",
       " ('dark', 3),\n",
       " ('marching', 3),\n",
       " ('calling', 3),\n",
       " ('introduce', 3),\n",
       " ('announced', 3),\n",
       " ('arrival', 3),\n",
       " ('mentioned', 3),\n",
       " ('romilda', 3),\n",
       " ('vane', 3),\n",
       " ('word', 3),\n",
       " ('sped', 3),\n",
       " ('witches', 3),\n",
       " ('armor', 3),\n",
       " ('steps', 3),\n",
       " ('girl', 3),\n",
       " ('ariana', 3),\n",
       " ('severus', 3),\n",
       " ('bloody', 2),\n",
       " ('control', 2),\n",
       " ('prefects', 2),\n",
       " ('glowing', 2),\n",
       " ('shadows', 2),\n",
       " ('wasn', 2),\n",
       " ('easily', 2),\n",
       " ('angry', 2),\n",
       " ('train', 2),\n",
       " ('tomorrow', 2),\n",
       " ('nighttime', 2),\n",
       " ('shoulders', 2),\n",
       " ('toppled', 2),\n",
       " ('guess', 2),\n",
       " ('stuck', 2),\n",
       " ('clock', 2),\n",
       " ('burst', 2),\n",
       " ('clearly', 2),\n",
       " ('cut', 2),\n",
       " ('arrive', 2),\n",
       " ('arms', 2),\n",
       " ('better', 2),\n",
       " ('camera', 2),\n",
       " ('longbottom', 2),\n",
       " ('held', 2),\n",
       " ('winking', 2),\n",
       " ('shoulder', 2),\n",
       " ('quidditch', 2),\n",
       " ('practice', 2),\n",
       " ('watched', 2),\n",
       " ('board', 2),\n",
       " ('justin', 2),\n",
       " ('usually', 2),\n",
       " ('snow', 2),\n",
       " ('counting', 2),\n",
       " ('distant', 2),\n",
       " ('sounds', 2),\n",
       " ('throwing', 2),\n",
       " ('teachers', 2),\n",
       " ('slid', 2),\n",
       " ('miserable', 2),\n",
       " ('crossed', 2),\n",
       " ('lot', 2),\n",
       " ('marble', 2),\n",
       " ('girls', 2),\n",
       " ('hasn', 2),\n",
       " ('glad', 2),\n",
       " ('weren', 2),\n",
       " ('second', 2),\n",
       " ('work', 2),\n",
       " ('turn', 2),\n",
       " ('feast', 2),\n",
       " ('glancing', 2),\n",
       " ('isn', 2),\n",
       " ('curiously', 2),\n",
       " ('closer', 2),\n",
       " ('torn', 2),\n",
       " ('whisper', 2),\n",
       " ('moving', 2),\n",
       " ('hiding', 2),\n",
       " ('ripped', 2),\n",
       " ('firmly', 2),\n",
       " ('cloaks', 2),\n",
       " ('oak', 2),\n",
       " ('christmas', 2),\n",
       " ('party', 2),\n",
       " ('previous', 2),\n",
       " ('headmasters', 2),\n",
       " ('sat', 2),\n",
       " ('carried', 2),\n",
       " ('staring', 2),\n",
       " ('stared', 2),\n",
       " ('delighted', 2),\n",
       " ('match', 2),\n",
       " ('getting', 2),\n",
       " ('weasley', 2),\n",
       " ('shaking', 2),\n",
       " ('breath', 2),\n",
       " ('em', 2),\n",
       " ('landing', 2),\n",
       " ('laughing', 2),\n",
       " ('bit', 2),\n",
       " ('concealed', 2),\n",
       " ('peeves', 2),\n",
       " ('ears', 2),\n",
       " ('join', 2),\n",
       " ('potter', 2),\n",
       " ('say', 2),\n",
       " ('annoyed', 2),\n",
       " ('laugh', 2),\n",
       " ('year', 2),\n",
       " ('demanded', 2),\n",
       " ('seen', 2),\n",
       " ('friend', 2),\n",
       " ('yell', 2),\n",
       " ('woke', 2),\n",
       " ('shown', 2),\n",
       " ('mood', 2),\n",
       " ('quite', 2),\n",
       " ('life', 2),\n",
       " ('rolling', 2),\n",
       " ('wake', 2),\n",
       " ('led', 2),\n",
       " ('added', 2),\n",
       " ('hear', 2),\n",
       " ('free', 2),\n",
       " ('idea', 2),\n",
       " ('scowled', 2),\n",
       " ('halt', 2),\n",
       " ('correct', 2),\n",
       " ('revealing', 2),\n",
       " ('talk', 2),\n",
       " ('kind', 2),\n",
       " ('light', 2),\n",
       " ('kept', 2),\n",
       " ('allowed', 2),\n",
       " ('sound', 2),\n",
       " ('dead', 2),\n",
       " ('cold', 2),\n",
       " ('covered', 2),\n",
       " ('hastily', 2),\n",
       " ('feet', 2),\n",
       " ('seamus', 2),\n",
       " ('panting', 2),\n",
       " ('view', 2),\n",
       " ('st', 2),\n",
       " ('mungo', 2),\n",
       " ('looks', 2),\n",
       " ('bad', 2),\n",
       " ('clever', 2),\n",
       " ('pointed', 2),\n",
       " ('slytherin', 2),\n",
       " ('longer', 2),\n",
       " ('message', 2),\n",
       " ('traveling', 2),\n",
       " ('study', 2),\n",
       " ('healer', 2),\n",
       " ('sideways', 2),\n",
       " ('queue', 2),\n",
       " ('shining', 2),\n",
       " ('middle', 2),\n",
       " ('amused', 2),\n",
       " ('apparently', 2),\n",
       " ('red', 2),\n",
       " ('forest', 2),\n",
       " ('dawn', 2),\n",
       " ('arguing', 2),\n",
       " ('broken', 2),\n",
       " ('pictures', 2),\n",
       " ('dippet', 2),\n",
       " ('owe', 2),\n",
       " ('hurtled', 2),\n",
       " ('ignoring', 2),\n",
       " ('tonight', 2),\n",
       " ('naturally', 2),\n",
       " ('uncomfortable', 2),\n",
       " ('curly', 2),\n",
       " ('digging', 2),\n",
       " ('ear', 2),\n",
       " ('wish', 2),\n",
       " ('image', 2),\n",
       " ('number', 2),\n",
       " ('boy', 2),\n",
       " ('joined', 2),\n",
       " ('okay', 2),\n",
       " ('feeling', 2),\n",
       " ('darted', 2),\n",
       " ('late', 2),\n",
       " ('make', 2),\n",
       " ('merely', 2),\n",
       " ('sort', 2),\n",
       " ('leapt', 2),\n",
       " ('aside', 2),\n",
       " ('fell', 2),\n",
       " ('cried', 2),\n",
       " ('help', 2),\n",
       " ('hit', 2),\n",
       " ('small', 2),\n",
       " ('group', 2),\n",
       " ('understood', 2),\n",
       " ('chest', 2),\n",
       " ('folded', 2),\n",
       " ('thinking', 2),\n",
       " ('hogwarts:', 2),\n",
       " ('moon', 2),\n",
       " ('spectacles', 2),\n",
       " ('died', 2),\n",
       " ('forgotten', 2),\n",
       " ('bring', 2),\n",
       " ('departure', 2),\n",
       " ('gruffly', 2),\n",
       " ('larger', 2),\n",
       " ('fang', 2),\n",
       " ('galloping', 2),\n",
       " ('alongside', 2),\n",
       " ('wizards', 2),\n",
       " ('mudblood', 2),\n",
       " ('scene', 2),\n",
       " ('tears', 2),\n",
       " ('voldemort', 2),\n",
       " ('granger', 2),\n",
       " ('sword', 2),\n",
       " ('baron', 1),\n",
       " ('round', 1),\n",
       " ('turning', 1),\n",
       " ('hunched', 1),\n",
       " ('nearest', 1),\n",
       " ('lamp', 1),\n",
       " ('hissing', 1),\n",
       " ('remember', 1),\n",
       " ('home', 1),\n",
       " ('facing', 1),\n",
       " ('cared', 1),\n",
       " ('space', 1),\n",
       " ('possible', 1),\n",
       " ('monster', 1),\n",
       " ('earth', 1),\n",
       " ('hanging', 1),\n",
       " ('flushed', 1),\n",
       " ('sweaty', 1),\n",
       " ('faces', 1),\n",
       " ('panted', 1),\n",
       " ('collapsed', 1),\n",
       " ('trembling', 1),\n",
       " ('saving', 1),\n",
       " ('hadn', 1),\n",
       " ('locked', 1),\n",
       " ('reminded', 1),\n",
       " ('noisy', 1),\n",
       " ('quickly', 1),\n",
       " ('play', 1),\n",
       " ('legs', 1),\n",
       " ('recognized', 1),\n",
       " ('locker', 1),\n",
       " ('curse', 1),\n",
       " ('midnight', 1),\n",
       " ('tail', 1),\n",
       " ('wailed', 1),\n",
       " ('desperate', 1),\n",
       " ('exploded', 1),\n",
       " ('idiot', 1),\n",
       " ('words', 1),\n",
       " ('sudden', 1),\n",
       " ('storm', 1),\n",
       " ('clapping', 1),\n",
       " ('lopsided', 1),\n",
       " ('tables', 1),\n",
       " ('squashy', 1),\n",
       " ('pull', 1),\n",
       " ('brilliant', 1),\n",
       " ('smirking', 1),\n",
       " ('mr', 1),\n",
       " ('beaming', 1),\n",
       " ('double', 1),\n",
       " ('sign', 1),\n",
       " ('fumbled', 1),\n",
       " ('bell', 1),\n",
       " ('rang', 1),\n",
       " ('picked', 1),\n",
       " ('copy', 1),\n",
       " ('gilderoy', 1),\n",
       " ('order', 1),\n",
       " ('merlin', 1),\n",
       " ('class', 1),\n",
       " ('member', 1),\n",
       " ('nimbus', 1),\n",
       " ('thousand', 1),\n",
       " ('clatter', 1),\n",
       " ('dashing', 1),\n",
       " ('swinging', 1),\n",
       " ('hurry', 1),\n",
       " ('wow', 1),\n",
       " ('game', 1),\n",
       " ('knight', 1),\n",
       " ('horse', 1),\n",
       " ('dragged', 1),\n",
       " ('important', 1),\n",
       " ('wondering', 1),\n",
       " ('darker', 1),\n",
       " ('swirling', 1),\n",
       " ('attacks', 1),\n",
       " ('urge', 1),\n",
       " ('thinks', 1),\n",
       " ('somewhat', 1),\n",
       " ('awkwardly', 1),\n",
       " ('ghost', 1),\n",
       " ('ravenclaw', 1),\n",
       " ('seizing', 1),\n",
       " ('difficult', 1),\n",
       " ('journey', 1),\n",
       " ('dodging', 1),\n",
       " ('weasleys', 1),\n",
       " ('darkness', 1),\n",
       " ('falling', 1),\n",
       " ('activity', 1),\n",
       " ('tired', 1),\n",
       " ('sadly', 1),\n",
       " ('remembering', 1),\n",
       " ('divided', 1),\n",
       " ('separate', 1),\n",
       " ('dementors', 1),\n",
       " ('things', 1),\n",
       " ('meet', 1),\n",
       " ('anybody', 1),\n",
       " ('entirely', 1),\n",
       " ('sure', 1),\n",
       " ('october', 1),\n",
       " ('halloween', 1),\n",
       " ('excellent', 1),\n",
       " ('zonko', 1),\n",
       " ('jerking', 1),\n",
       " ('chattering', 1),\n",
       " ('older', 1),\n",
       " ('forehead', 1),\n",
       " ('library', 1),\n",
       " ('choice', 1),\n",
       " ('waking', 1),\n",
       " ('grumpily', 1),\n",
       " ('checked', 1),\n",
       " ('starting', 1),\n",
       " ('discussing', 1),\n",
       " ('dropped', 1),\n",
       " ('nervously', 1),\n",
       " ('usual', 1),\n",
       " ('path', 1),\n",
       " ('ended', 1),\n",
       " ('jammed', 1),\n",
       " ('peered', 1),\n",
       " ('bustling', 1),\n",
       " ('importantly', 1),\n",
       " ('arrived', 1),\n",
       " ('sweeping', 1),\n",
       " ('squeezed', 1),\n",
       " ('moved', 1),\n",
       " ('grabbed', 1),\n",
       " ('arm', 1),\n",
       " ('slashed', 1),\n",
       " ('littered', 1),\n",
       " ('map', 1),\n",
       " ('replaced', 1),\n",
       " ('happy', 1),\n",
       " ('spent', 1),\n",
       " ('sneaking', 1),\n",
       " ('breakfast', 1),\n",
       " ('yellow', 1),\n",
       " ('men', 1),\n",
       " ('enjoying', 1),\n",
       " ('couple', 1),\n",
       " ('handle', 1),\n",
       " ('shiny', 1),\n",
       " ('pointless', 1),\n",
       " ('angle', 1),\n",
       " ('accompanied', 1),\n",
       " ('certain', 1),\n",
       " ('informed', 1),\n",
       " ('heel', 1),\n",
       " ('firebolt', 1),\n",
       " ('tin', 1),\n",
       " ('high', 1),\n",
       " ('finish', 1),\n",
       " ('clutched', 1),\n",
       " ('startled', 1),\n",
       " ('eat', 1),\n",
       " ('nightmare', 1),\n",
       " ('telling', 1),\n",
       " ('slammed', 1),\n",
       " ('furiously', 1),\n",
       " ('knife', 1),\n",
       " ('ridiculous', 1),\n",
       " ('possibly', 1),\n",
       " ('gotten', 1),\n",
       " ('finger', 1),\n",
       " ('glaring', 1),\n",
       " ('suspiciously', 1),\n",
       " ('listened', 1),\n",
       " ('piece', 1),\n",
       " ('paper', 1),\n",
       " ('stunned', 1),\n",
       " ('white', 1),\n",
       " ('person', 1),\n",
       " ('mouse', 1),\n",
       " ('holes', 1),\n",
       " ('fired', 1),\n",
       " ('lonely', 1),\n",
       " ('woken', 1),\n",
       " ('thoughtfully', 1),\n",
       " ('kill', 1),\n",
       " ('total', 1),\n",
       " ('furious', 1),\n",
       " ('security', 1),\n",
       " ('fast', 1),\n",
       " ('asleep', 1),\n",
       " ('resting', 1),\n",
       " ('joking', 1),\n",
       " ('heading', 1),\n",
       " ('freedom', 1),\n",
       " ('sentence', 1),\n",
       " ('strode', 1),\n",
       " ('banging', 1),\n",
       " ('prefect', 1),\n",
       " ('crackling', 1),\n",
       " ('carrying', 1),\n",
       " ('fine', 1),\n",
       " ('worry', 1),\n",
       " ('feels', 1),\n",
       " ('normal', 1),\n",
       " ('briefly', 1),\n",
       " ('blast', 1),\n",
       " ('knocked', 1),\n",
       " ('backward', 1),\n",
       " ('wrenched', 1),\n",
       " ('dozen', 1),\n",
       " ('allow', 1),\n",
       " ('cornered', 1),\n",
       " ('brothers', 1),\n",
       " ('frantically', 1),\n",
       " ('resolutely', 1),\n",
       " ('hello', 1),\n",
       " ('instead', 1),\n",
       " ('far', 1),\n",
       " ('badges', 1),\n",
       " ('minute', 1),\n",
       " ('keeping', 1),\n",
       " ('dare', 1),\n",
       " ('slow', 1),\n",
       " ('gasped', 1),\n",
       " ('muttered', 1),\n",
       " ('sleepily', 1),\n",
       " ('opening', 1),\n",
       " ('fourth', 1),\n",
       " ('bowed', 1),\n",
       " ('parvati', 1),\n",
       " ('action', 1),\n",
       " ('wishing', 1),\n",
       " ('winked', 1),\n",
       " ('o', 1),\n",
       " ('fool', 1),\n",
       " ('cho', 1),\n",
       " ('lights', 1),\n",
       " ('irritated', 1),\n",
       " ('dragons', 1),\n",
       " ('bye', 1),\n",
       " ('straight', 1),\n",
       " ('tortured', 1),\n",
       " ('size', 1),\n",
       " ('unpleasant', 1),\n",
       " ('month', 1),\n",
       " ('sticking', 1),\n",
       " ('charm', 1),\n",
       " ('quick', 1),\n",
       " ('bewildered', 1),\n",
       " ('earsplitting', 1),\n",
       " ('lupin', 1),\n",
       " ('calm', 1),\n",
       " ('kitchen', 1),\n",
       " ('seat', 1),\n",
       " ('obviously', 1),\n",
       " ('walking', 1),\n",
       " ('clattering', 1),\n",
       " ('chain', 1),\n",
       " ('deep', 1),\n",
       " ('orders', 1),\n",
       " ('foul', 1),\n",
       " ('hopefully', 1),\n",
       " ('parents', 1),\n",
       " ('sighed', 1),\n",
       " ('wouldn', 1),\n",
       " ('occasionally', 1),\n",
       " ('useful', 1),\n",
       " ('stuffed', 1),\n",
       " ('cage', 1),\n",
       " ('dragging', 1),\n",
       " ('trunk', 1),\n",
       " ('howling', 1),\n",
       " ('rage', 1),\n",
       " ('bothering', 1),\n",
       " ('close', 1),\n",
       " ('bound', 1),\n",
       " ('events', 1),\n",
       " ('graveyard', 1),\n",
       " ('er', 1),\n",
       " ('glumly', 1),\n",
       " ('positively', 1),\n",
       " ('alarmed', 1),\n",
       " ('cabin', 1),\n",
       " ('chairs', 1),\n",
       " ('sense', 1),\n",
       " ('stuff', 1),\n",
       " ('working', 1),\n",
       " ('carefully', 1),\n",
       " ('owlery', 1),\n",
       " ('headless', 1),\n",
       " ('nick', 1),\n",
       " ('drifting', 1),\n",
       " ('coolly', 1),\n",
       " ('hour', 1),\n",
       " ('lousy', 1),\n",
       " ('disheveled', 1),\n",
       " ('realize', 1),\n",
       " ('happen', 1),\n",
       " ('fair', 1),\n",
       " ('giggling', 1),\n",
       " ('madly', 1),\n",
       " ('fashioned', 1),\n",
       " ('senses', 1),\n",
       " ('early', 1),\n",
       " ('ravenclaws', 1),\n",
       " ('west', 1),\n",
       " ('finally', 1),\n",
       " ('creaking', 1),\n",
       " ('pale', 1),\n",
       " ('tracks', 1),\n",
       " ('elf', 1),\n",
       " ('hats', 1),\n",
       " ('defensively', 1),\n",
       " ('clicked', 1),\n",
       " ('tongue', 1),\n",
       " ('grown', 1),\n",
       " ('crouch', 1),\n",
       " ('prevent', 1),\n",
       " ('panic', 1),\n",
       " ('strange', 1),\n",
       " ('instrument', 1),\n",
       " ('shout', 1),\n",
       " ('reappeared', 1),\n",
       " ('news', 1),\n",
       " ('doesn', 1),\n",
       " ('blood', 1),\n",
       " ('coughing', 1),\n",
       " ('armchair', 1),\n",
       " ('thank', 1),\n",
       " ('minerva', 1),\n",
       " ('blue', 1),\n",
       " ('quivered', 1),\n",
       " ('marched', 1),\n",
       " ('beard', 1),\n",
       " ('colors', 1),\n",
       " ('jerk', 1),\n",
       " ('wide', 1),\n",
       " ('giving', 1),\n",
       " ('fake', 1),\n",
       " ('eyeing', 1),\n",
       " ('apprehensively', 1),\n",
       " ('destroyed', 1),\n",
       " ('family', 1),\n",
       " ('knows', 1),\n",
       " ('destroy', 1),\n",
       " ('before:', 1),\n",
       " ('issuing', 1),\n",
       " ('bored', 1),\n",
       " ('disappeared', 1),\n",
       " ('antidotes', 1),\n",
       " ('anti', 1),\n",
       " ('unless', 1),\n",
       " ('qualified', 1),\n",
       " ('witch', 1),\n",
       " ('young', 1),\n",
       " ('performing', 1),\n",
       " ('spot', 1),\n",
       " ('crystal', 1),\n",
       " ('bubbles', 1),\n",
       " ('ceiling', 1),\n",
       " ('vicious', 1),\n",
       " ('pace', 1),\n",
       " ('beds', 1),\n",
       " ('brain', 1),\n",
       " ('questions', 1),\n",
       " ('dreadful', 1),\n",
       " ('ideas', 1),\n",
       " ('snake', 1),\n",
       " ('animagus', 1),\n",
       " ('leaning', 1),\n",
       " ('watching', 1),\n",
       " ('squinting', 1),\n",
       " ('outline', 1),\n",
       " ('occurred', 1),\n",
       " ('probably', 1),\n",
       " ('case', 1),\n",
       " ('attacked', 1),\n",
       " ('dad', 1),\n",
       " ('visited', 1),\n",
       " ('comfort', 1),\n",
       " ('pile', 1),\n",
       " ('rat', 1),\n",
       " ('dropping', 1),\n",
       " ('passing', 1),\n",
       " ('says', 1),\n",
       " ('raising', 1),\n",
       " ('eyebrows', 1),\n",
       " ('interesting', 1),\n",
       " ('roared', 1),\n",
       " ('nosed', 1),\n",
       " ('ministry', 1),\n",
       " ('creatures', 1),\n",
       " ('examining', 1),\n",
       " ('unicorns', 1),\n",
       " ('thoroughly', 1),\n",
       " ('tempered', 1),\n",
       " ('runes', 1),\n",
       " ('exams', 1),\n",
       " ('celebration', 1),\n",
       " ('approaching', 1),\n",
       " ('occasional', 1),\n",
       " ('grunt', 1),\n",
       " ('sleeping', 1),\n",
       " ('surroundings', 1),\n",
       " ('reflected', 1),\n",
       " ('feelings', 1),\n",
       " ('pain', 1),\n",
       " ('quiet', 1),\n",
       " ('shattered', 1),\n",
       " ('pieces', 1),\n",
       " ('yells', 1),\n",
       " ('anger', 1),\n",
       " ('fright', 1),\n",
       " ('snatching', 1),\n",
       " ('hitting', 1),\n",
       " ('noticing', 1),\n",
       " ('start', 1),\n",
       " ('cutting', 1),\n",
       " ('telephone', 1),\n",
       " ('heart', 1),\n",
       " ('sank', 1),\n",
       " ('afraid', 1),\n",
       " ('hoping', 1),\n",
       " ('speak', 1),\n",
       " ('dreaming', 1),\n",
       " ('cough', 1),\n",
       " ('magic', 1),\n",
       " ('strain', 1),\n",
       " ('caused', 1),\n",
       " ('utterly', 1),\n",
       " ('terrified', 1),\n",
       " ('self', 1),\n",
       " ('bounced', 1),\n",
       " ('shaken', 1),\n",
       " ('remained', 1),\n",
       " ('encounter', 1),\n",
       " ('given', 1),\n",
       " ('private', 1),\n",
       " ('proved', 1),\n",
       " ('impossible', 1),\n",
       " ('remove', 1),\n",
       " ('art', 1),\n",
       " ('happened', 1),\n",
       " ('ago', 1),\n",
       " ('wet', 1),\n",
       " ('state', 1),\n",
       " ('considerable', 1),\n",
       " ('course', 1),\n",
       " ('busy', 1),\n",
       " ('quill', 1),\n",
       " ('catching', 1),\n",
       " ('finishing', 1),\n",
       " ('letter', 1),\n",
       " ('luck', 1),\n",
       " ('maybe', 1),\n",
       " ('scrimgeour', 1),\n",
       " ('success', 1),\n",
       " ('subsided', 1),\n",
       " ('suddenly', 1),\n",
       " ('official', 1),\n",
       " ('muggles', 1),\n",
       " ('meeting', 1),\n",
       " ('enchantment', 1),\n",
       " ('ensure', 1),\n",
       " ('owned', 1),\n",
       " ('pureblood', 1),\n",
       " ('vivid', 1),\n",
       " ('shrieking', 1),\n",
       " ('spitting', 1),\n",
       " ('flashed', 1),\n",
       " ('snapped', 1),\n",
       " ('particularly', 1),\n",
       " ('minuscule', 1),\n",
       " ('muttering', 1),\n",
       " ('promptly', 1),\n",
       " ('scarlet', 1),\n",
       " ('hope', 1),\n",
       " ('goes', 1),\n",
       " ('pair', 1),\n",
       " ('leave', 1),\n",
       " ('proceeded', 1),\n",
       " ('step', 1),\n",
       " ('statue', 1),\n",
       " ('trelawney', 1),\n",
       " ('smelled', 1),\n",
       " ('damp', 1),\n",
       " ('stalked', 1),\n",
       " ('undoubtedly', 1),\n",
       " ('pause', 1),\n",
       " ('worst', 1),\n",
       " ('darkly', 1),\n",
       " ('evening', 1),\n",
       " ('soon', 1),\n",
       " ('sinking', 1),\n",
       " ('mane', 1),\n",
       " ('bushy', 1),\n",
       " ('brown', 1),\n",
       " ('whipping', 1),\n",
       " ('unlocked', 1),\n",
       " ('classroom', 1),\n",
       " ('hi', 1),\n",
       " ('fancy', 1),\n",
       " ('thanks', 1),\n",
       " ('precisely', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(item, value) for item, value in contexts[\"portrait\"].items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the word 'ghost'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('harry', 52),\n",
       " ('said', 36),\n",
       " ('nick', 22),\n",
       " ('t', 20),\n",
       " ('nearly', 20),\n",
       " ('headless', 18),\n",
       " ('know', 15),\n",
       " ('looked', 12),\n",
       " ('ron', 10),\n",
       " ('saw', 9),\n",
       " ('d', 9),\n",
       " ('ve', 9),\n",
       " ('got', 8),\n",
       " ('gryffindor', 8),\n",
       " ('years', 7),\n",
       " ('bloody', 7),\n",
       " ('baron', 7),\n",
       " ('wand', 7),\n",
       " ('cedric', 7),\n",
       " ('dumbledore', 7),\n",
       " ('just', 6),\n",
       " ('don', 6),\n",
       " ('think', 6),\n",
       " ('told', 6),\n",
       " ('slytherin', 6),\n",
       " ('staring', 6),\n",
       " ('eyes', 6),\n",
       " ('professor', 6),\n",
       " ('passed', 6),\n",
       " ('like', 6),\n",
       " ('hermione', 6),\n",
       " ('gray', 6),\n",
       " ('ruff', 5),\n",
       " ('suddenly', 5),\n",
       " ('table', 5),\n",
       " ('potter', 5),\n",
       " ('giving', 5),\n",
       " ('began', 5),\n",
       " ('tower', 5),\n",
       " ('magic', 5),\n",
       " ('binns', 5),\n",
       " ('light', 5),\n",
       " ('held', 5),\n",
       " ('head', 5),\n",
       " ('little', 5),\n",
       " ('solid', 5),\n",
       " ('really', 4),\n",
       " ('hufflepuff', 4),\n",
       " ('hat', 4),\n",
       " ('opposite', 4),\n",
       " ('seen', 4),\n",
       " ('arm', 4),\n",
       " ('need', 4),\n",
       " ('sir', 4),\n",
       " ('gaunt', 4),\n",
       " ('face', 4),\n",
       " ('silver', 4),\n",
       " ('corridor', 4),\n",
       " ('covered', 4),\n",
       " ('wide', 4),\n",
       " ('oh', 4),\n",
       " ('asked', 4),\n",
       " ('away', 4),\n",
       " ('myrtle', 4),\n",
       " ('hair', 4),\n",
       " ('thing', 4),\n",
       " ('hogwarts', 4),\n",
       " ('white', 4),\n",
       " ('stood', 4),\n",
       " ('voldemort', 4),\n",
       " ('end', 4),\n",
       " ('neville', 4),\n",
       " ('come', 4),\n",
       " ('haven', 3),\n",
       " ('peeves', 3),\n",
       " ('say', 3),\n",
       " ('doing', 3),\n",
       " ('wearing', 3),\n",
       " ('fat', 3),\n",
       " ('sat', 3),\n",
       " ('horrible', 3),\n",
       " ('plate', 3),\n",
       " ('good', 3),\n",
       " ('course', 3),\n",
       " ('seamus', 3),\n",
       " ('sitting', 3),\n",
       " ('history', 3),\n",
       " ('taught', 3),\n",
       " ('let', 3),\n",
       " ('direction', 3),\n",
       " ('moaning', 3),\n",
       " ('came', 3),\n",
       " ('help', 3),\n",
       " ('hedwig', 3),\n",
       " ('window', 3),\n",
       " ('man', 3),\n",
       " ('talking', 3),\n",
       " ('wasn', 3),\n",
       " ('surprised', 3),\n",
       " ('drifted', 3),\n",
       " ('mean', 3),\n",
       " ('girl', 3),\n",
       " ('wall', 3),\n",
       " ('floor', 3),\n",
       " ('large', 3),\n",
       " ('room', 3),\n",
       " ('attack', 3),\n",
       " ('crash', 3),\n",
       " ('gryffindors', 3),\n",
       " ('ravenclaw', 3),\n",
       " ('riddle', 3),\n",
       " ('diary', 3),\n",
       " ('diggory', 3),\n",
       " ('looking', 3),\n",
       " ('heard', 3),\n",
       " ('pain', 3),\n",
       " ('body', 3),\n",
       " ('alive', 3),\n",
       " ('gone', 3),\n",
       " ('golden', 3),\n",
       " ('emerged', 3),\n",
       " ('wizard', 3),\n",
       " ('malfoy', 3),\n",
       " ('sure', 3),\n",
       " ('given', 2),\n",
       " ('chances', 2),\n",
       " ('deserves', 2),\n",
       " ('gives', 2),\n",
       " ('bad', 2),\n",
       " ('noticed', 2),\n",
       " ('answered', 2),\n",
       " ('students', 2),\n",
       " ('clapped', 2),\n",
       " ('susan', 2),\n",
       " ('shouted', 2),\n",
       " ('weasley', 2),\n",
       " ('twins', 2),\n",
       " ('earlier', 2),\n",
       " ('patted', 2),\n",
       " ('sudden', 2),\n",
       " ('bit', 2),\n",
       " ('does', 2),\n",
       " ('sadly', 2),\n",
       " ('cut', 2),\n",
       " ('steak', 2),\n",
       " ('eaten', 2),\n",
       " ('myself', 2),\n",
       " ('brothers', 2),\n",
       " ('haired', 2),\n",
       " ('finnigan', 2),\n",
       " ('blank', 2),\n",
       " ('used', 2),\n",
       " ('boring', 2),\n",
       " ('class', 2),\n",
       " ('old', 2),\n",
       " ('m', 2),\n",
       " ('tall', 2),\n",
       " ('gliding', 2),\n",
       " ('started', 2),\n",
       " ('free', 2),\n",
       " ('soft', 2),\n",
       " ('ahead', 2),\n",
       " ('wings', 2),\n",
       " ('moment', 2),\n",
       " ('soared', 2),\n",
       " ('impatiently', 2),\n",
       " ('warning', 2),\n",
       " ('somebody', 2),\n",
       " ('forehead', 2),\n",
       " ('watched', 2),\n",
       " ('crouched', 2),\n",
       " ('walked', 2),\n",
       " ('er', 2),\n",
       " ('horses', 2),\n",
       " ('pack', 2),\n",
       " ('bearded', 2),\n",
       " ('position', 2),\n",
       " ('blowing', 2),\n",
       " ('horn', 2),\n",
       " ('leapt', 2),\n",
       " ('lifted', 2),\n",
       " ('high', 2),\n",
       " ('laughed', 2),\n",
       " ('subject', 2),\n",
       " ('teacher', 2),\n",
       " ('happened', 2),\n",
       " ('way', 2),\n",
       " ('filled', 2),\n",
       " ('safe', 2),\n",
       " ('door', 2),\n",
       " ('flew', 2),\n",
       " ('people', 2),\n",
       " ('dead', 2),\n",
       " ('seats', 2),\n",
       " ('counting', 2),\n",
       " ('school', 2),\n",
       " ('ago', 2),\n",
       " ('memory', 2),\n",
       " ('quietly', 2),\n",
       " ('rolling', 2),\n",
       " ('sleeves', 2),\n",
       " ('turn', 2),\n",
       " ('black', 2),\n",
       " ('quickly', 2),\n",
       " ('seeing', 2),\n",
       " ('things', 2),\n",
       " ('wands', 2),\n",
       " ('shadow', 2),\n",
       " ('skull', 2),\n",
       " ('mr', 2),\n",
       " ('vanished', 2),\n",
       " ('rest', 2),\n",
       " ('particularly', 2),\n",
       " ('chance', 2),\n",
       " ('opinion', 2),\n",
       " ('foot', 2),\n",
       " ('running', 2),\n",
       " ('clutching', 2),\n",
       " ('fear', 2),\n",
       " ('going', 2),\n",
       " ('water', 2),\n",
       " ('shock', 2),\n",
       " ('ripped', 2),\n",
       " ('spirit', 2),\n",
       " ('hand', 2),\n",
       " ('tip', 2),\n",
       " ('tightly', 2),\n",
       " ('thread', 2),\n",
       " ('remained', 2),\n",
       " ('itself', 2),\n",
       " ('correct', 2),\n",
       " ('spoke', 2),\n",
       " ('shaking', 2),\n",
       " ('past', 2),\n",
       " ('feels', 2),\n",
       " ('voice', 2),\n",
       " ('hey', 2),\n",
       " ('cloak', 2),\n",
       " ('floating', 2),\n",
       " ('board', 2),\n",
       " ('long', 2),\n",
       " ('tried', 2),\n",
       " ('tell', 2),\n",
       " ('difference', 2),\n",
       " ('lying', 2),\n",
       " ('tomb', 2),\n",
       " ('dunno', 2),\n",
       " ('wouldn', 2),\n",
       " ('felt', 2),\n",
       " ('length', 2),\n",
       " ('times', 2),\n",
       " ('spoken', 2),\n",
       " ('lady', 2),\n",
       " ('nodded', 2),\n",
       " ('did', 2),\n",
       " ('speak', 2),\n",
       " ('fred', 2),\n",
       " ('new', 1),\n",
       " ('right', 1),\n",
       " ('cheered', 1),\n",
       " ('hannah', 1),\n",
       " ('went', 1),\n",
       " ('sit', 1),\n",
       " ('waving', 1),\n",
       " ('merrily', 1),\n",
       " ('bones', 1),\n",
       " ('yelled', 1),\n",
       " ('feeling', 1),\n",
       " ('plunged', 1),\n",
       " ('eat', 1),\n",
       " ('delicious', 1),\n",
       " ('look', 1),\n",
       " ('watching', 1),\n",
       " ('miss', 1),\n",
       " ('introduced', 1),\n",
       " ('service', 1),\n",
       " ('prefer', 1),\n",
       " ('stiffly', 1),\n",
       " ('interrupted', 1),\n",
       " ('winning', 1),\n",
       " ('slytherins', 1),\n",
       " ('cup', 1),\n",
       " ('row', 1),\n",
       " ('robes', 1),\n",
       " ('stained', 1),\n",
       " ('blood', 1),\n",
       " ('strange', 1),\n",
       " ('plants', 1),\n",
       " ('easily', 1),\n",
       " ('fallen', 1),\n",
       " ('asleep', 1),\n",
       " ('staffroom', 1),\n",
       " ('morning', 1),\n",
       " ('teach', 1),\n",
       " ('leaving', 1),\n",
       " ('freezing', 1),\n",
       " ('forget', 1),\n",
       " ('hissed', 1),\n",
       " ('witch', 1),\n",
       " ('stirring', 1),\n",
       " ('cauldrons', 1),\n",
       " ('wonderful', 1),\n",
       " ('week', 1),\n",
       " ('exam', 1),\n",
       " ('results', 1),\n",
       " ('quills', 1),\n",
       " ('roll', 1),\n",
       " ('parchment', 1),\n",
       " ('couldn', 1),\n",
       " ('whispered', 1),\n",
       " ('listened', 1),\n",
       " ('rustling', 1),\n",
       " ('clinking', 1),\n",
       " ('coming', 1),\n",
       " ('sounds', 1),\n",
       " ('moving', 1),\n",
       " ('reached', 1),\n",
       " ('later', 1),\n",
       " ('alongside', 1),\n",
       " ('story', 1),\n",
       " ('happening', 1),\n",
       " ('dobby', 1),\n",
       " ('deserted', 1),\n",
       " ('muttering', 1),\n",
       " ('breath', 1),\n",
       " ('gloomy', 1),\n",
       " ('ragged', 1),\n",
       " ('chains', 1),\n",
       " ('cheerful', 1),\n",
       " ('knight', 1),\n",
       " ('sticking', 1),\n",
       " ('ghosts', 1),\n",
       " ('died', 1),\n",
       " ('october', 1),\n",
       " ('amazed', 1),\n",
       " ('approached', 1),\n",
       " ('low', 1),\n",
       " ('mouth', 1),\n",
       " ('taste', 1),\n",
       " ('walk', 1),\n",
       " ('expect', 1),\n",
       " ('stronger', 1),\n",
       " ('flavor', 1),\n",
       " ('didn', 1),\n",
       " ('mind', 1),\n",
       " ('hello', 1),\n",
       " ('squat', 1),\n",
       " ('glided', 1),\n",
       " ('half', 1),\n",
       " ('hidden', 1),\n",
       " ('bitterly', 1),\n",
       " ('dungeon', 1),\n",
       " ('burst', 1),\n",
       " ('dozen', 1),\n",
       " ('wildly', 1),\n",
       " ('clap', 1),\n",
       " ('middle', 1),\n",
       " ('dance', 1),\n",
       " ('halted', 1),\n",
       " ('plunging', 1),\n",
       " ('air', 1),\n",
       " ('crowd', 1),\n",
       " ('strode', 1),\n",
       " ('schedule', 1),\n",
       " ('exciting', 1),\n",
       " ('classes', 1),\n",
       " ('entering', 1),\n",
       " ('blackboard', 1),\n",
       " ('ancient', 1),\n",
       " ('shriveled', 1),\n",
       " ('lungs', 1),\n",
       " ('stop', 1),\n",
       " ('screamed', 1),\n",
       " ('mortal', 1),\n",
       " ('run', 1),\n",
       " ('lives', 1),\n",
       " ('real', 1),\n",
       " ('panic', 1),\n",
       " ('curiously', 1),\n",
       " ('fate', 1),\n",
       " ('worry', 1),\n",
       " ('possibly', 1),\n",
       " ('terrible', 1),\n",
       " ('power', 1),\n",
       " ('harm', 1),\n",
       " ('book', 1),\n",
       " ('awkwardly', 1),\n",
       " ('portrait', 1),\n",
       " ('hole', 1),\n",
       " ('immediately', 1),\n",
       " ('friend', 1),\n",
       " ('lee', 1),\n",
       " ('jordan', 1),\n",
       " ('fingers', 1),\n",
       " ('stand', 1),\n",
       " ('lockhart', 1),\n",
       " ('getting', 1),\n",
       " ('feet', 1),\n",
       " ('ways', 1),\n",
       " ('aside', 1),\n",
       " ('pipe', 1),\n",
       " ('miles', 1),\n",
       " ('weird', 1),\n",
       " ('misty', 1),\n",
       " ('shining', 1),\n",
       " ('day', 1),\n",
       " ('older', 1),\n",
       " ('sixteen', 1),\n",
       " ('uncertainly', 1),\n",
       " ('fifty', 1),\n",
       " ('popped', 1),\n",
       " ('walls', 1),\n",
       " ('tables', 1),\n",
       " ('great', 1),\n",
       " ('success', 1),\n",
       " ('pleasant', 1),\n",
       " ('evening', 1),\n",
       " ('mood', 1),\n",
       " ('lupin', 1),\n",
       " ('forgive', 1),\n",
       " ('believing', 1),\n",
       " ('spy', 1),\n",
       " ('grin', 1),\n",
       " ('flitted', 1),\n",
       " ('shall', 1),\n",
       " ('kill', 1),\n",
       " ('dad', 1),\n",
       " ('maybe', 1),\n",
       " ('met', 1),\n",
       " ('mere', 1),\n",
       " ('green', 1),\n",
       " ('spell', 1),\n",
       " ('smoke', 1),\n",
       " ('hufflepuffs', 1),\n",
       " ('far', 1),\n",
       " ('hall', 1),\n",
       " ('pearly', 1),\n",
       " ('dressed', 1),\n",
       " ('tonight', 1),\n",
       " ('usual', 1),\n",
       " ('question', 1),\n",
       " ('utterly', 1),\n",
       " ('food', 1),\n",
       " ('throwing', 1),\n",
       " ('silent', 1),\n",
       " ('person', 1),\n",
       " ('control', 1),\n",
       " ('store', 1),\n",
       " ('amused', 1),\n",
       " ('month', 1),\n",
       " ('ideas', 1),\n",
       " ('writing', 1),\n",
       " ('weekly', 1),\n",
       " ('goblin', 1),\n",
       " ('century', 1),\n",
       " ('amazing', 1),\n",
       " ('seriously', 1),\n",
       " ('goblet', 1),\n",
       " ('reckon', 1),\n",
       " ('trying', 1),\n",
       " ('weeks', 1),\n",
       " ('meeting', 1),\n",
       " ('noise', 1),\n",
       " ('loud', 1),\n",
       " ('wailing', 1),\n",
       " ('nearest', 1),\n",
       " ('party', 1),\n",
       " ('playing', 1),\n",
       " ('musical', 1),\n",
       " ('shut', 1),\n",
       " ('miracle', 1),\n",
       " ('sort', 1),\n",
       " ('extra', 1),\n",
       " ('concentrated', 1),\n",
       " ('supposed', 1),\n",
       " ('starting', 1),\n",
       " ('dancing', 1),\n",
       " ('champions', 1),\n",
       " ('suppose', 1),\n",
       " ('gloomily', 1),\n",
       " ('girls', 1),\n",
       " ('second', 1),\n",
       " ('putting', 1),\n",
       " ('swallowed', 1),\n",
       " ('considerable', 1),\n",
       " ('bubbles', 1),\n",
       " ('cross', 1),\n",
       " ('legged', 1),\n",
       " ('taps', 1),\n",
       " ('usually', 1),\n",
       " ('friends', 1),\n",
       " ('prepared', 1),\n",
       " ('anybody', 1),\n",
       " ('path', 1),\n",
       " ('leads', 1),\n",
       " ('goal', 1),\n",
       " ('red', 1),\n",
       " ('widened', 1),\n",
       " ('dense', 1),\n",
       " ('wormtail', 1),\n",
       " ('shouts', 1),\n",
       " ('larger', 1),\n",
       " ('kept', 1),\n",
       " ('squeezing', 1),\n",
       " ('narrow', 1),\n",
       " ('dream', 1),\n",
       " ('pushing', 1),\n",
       " ('himself', 1),\n",
       " ('fell', 1),\n",
       " ('surveyed', 1),\n",
       " ('web', 1),\n",
       " ('connected', 1),\n",
       " ('appearance', 1),\n",
       " ('guessing', 1),\n",
       " ('limped', 1),\n",
       " ('rustle', 1),\n",
       " ('small', 1),\n",
       " ('time', 1),\n",
       " ('snarled', 1),\n",
       " ('landed', 1),\n",
       " ('lightly', 1),\n",
       " ('cage', 1),\n",
       " ('work', 1),\n",
       " ('halfway', 1),\n",
       " ('house', 1),\n",
       " ('parvati', 1),\n",
       " ('patil', 1),\n",
       " ('lavender', 1),\n",
       " ('brown', 1),\n",
       " ('gave', 1),\n",
       " ('friendly', 1),\n",
       " ('leaning', 1),\n",
       " ('winced', 1),\n",
       " ('uncomfortable', 1),\n",
       " ('lean', 1),\n",
       " ('honor', 1),\n",
       " ('bound', 1),\n",
       " ('saying', 1),\n",
       " ('sorting', 1),\n",
       " ('glad', 1),\n",
       " ('reason', 1),\n",
       " ('common', 1),\n",
       " ('kind', 1),\n",
       " ('wheezy', 1),\n",
       " ('cause', 1),\n",
       " ('severe', 1),\n",
       " ('minutes', 1),\n",
       " ('warm', 1),\n",
       " ('drifting', 1),\n",
       " ('stuck', 1),\n",
       " ('revealing', 1),\n",
       " ('dangerously', 1),\n",
       " ('continued', 1),\n",
       " ('hesitated', 1),\n",
       " ('wizards', 1),\n",
       " ('year', 1),\n",
       " ('pansy', 1),\n",
       " ('indignantly', 1),\n",
       " ('smirk', 1),\n",
       " ('moved', 1),\n",
       " ('bigger', 1),\n",
       " ('better', 1),\n",
       " ('luggage', 1),\n",
       " ('rack', 1),\n",
       " ('conscious', 1),\n",
       " ('ginny', 1),\n",
       " ('dean', 1),\n",
       " ('listening', 1),\n",
       " ('bench', 1),\n",
       " ('darkly', 1),\n",
       " ('silvery', 1),\n",
       " ('mass', 1),\n",
       " ('rose', 1),\n",
       " ('revolving', 1),\n",
       " ('slowly', 1),\n",
       " ('pensieve', 1),\n",
       " ('completely', 1),\n",
       " ('curious', 1),\n",
       " ('circumstances', 1),\n",
       " ('brought', 1),\n",
       " ('yeh', 1),\n",
       " ('o', 1),\n",
       " ('governors', 1),\n",
       " ('hagrid', 1),\n",
       " ('stopped', 1),\n",
       " ('woman', 1),\n",
       " ('serenely', 1),\n",
       " ('resumed', 1),\n",
       " ('hoarse', 1),\n",
       " ('whisper', 1),\n",
       " ('corner', 1),\n",
       " ('apparently', 1),\n",
       " ('impression', 1),\n",
       " ('draco', 1),\n",
       " ('inside', 1),\n",
       " ('hour', 1),\n",
       " ('wondering', 1),\n",
       " ('paper', 1),\n",
       " ('snape', 1),\n",
       " ('bored', 1),\n",
       " ('fixed', 1),\n",
       " ('ask', 1),\n",
       " ('hastily', 1),\n",
       " ('recall', 1),\n",
       " ('night', 1),\n",
       " ('dark', 1),\n",
       " ('spells', 1),\n",
       " ('merely', 1),\n",
       " ('bidding', 1),\n",
       " ('trust', 1),\n",
       " ('aware', 1),\n",
       " ('departed', 1),\n",
       " ('soul', 1),\n",
       " ('left', 1),\n",
       " ('earth', 1),\n",
       " ('test', 1),\n",
       " ('boys', 1),\n",
       " ('bathroom', 1),\n",
       " ('risen', 1),\n",
       " ('toilet', 1),\n",
       " ('midair', 1),\n",
       " ('round', 1),\n",
       " ('glasses', 1),\n",
       " ('remembering', 1),\n",
       " ('words', 1),\n",
       " ('before:', 1),\n",
       " ('want', 1),\n",
       " ('tom', 1),\n",
       " ('death', 1),\n",
       " ('apparent', 1),\n",
       " ('expression', 1),\n",
       " ('explain', 1),\n",
       " ('sent', 1),\n",
       " ('knew', 1),\n",
       " ('precious', 1),\n",
       " ('attacked', 1),\n",
       " ('true', 1),\n",
       " ('destroyed', 1),\n",
       " ('thought', 1),\n",
       " ('feel', 1),\n",
       " ('surely', 1),\n",
       " ('horcruxes', 1),\n",
       " ('paced', 1),\n",
       " ('luna', 1),\n",
       " ('checking', 1),\n",
       " ('marauder', 1),\n",
       " ('map', 1),\n",
       " ('permitted', 1),\n",
       " ('twice', 1),\n",
       " ('pausing', 1),\n",
       " ('allow', 1),\n",
       " ('pass', 1),\n",
       " ('drawing', 1),\n",
       " ('attention', 1),\n",
       " ('expected', 1),\n",
       " ('encounter', 1),\n",
       " ('worst', 1),\n",
       " ('forced', 1),\n",
       " ('finally', 1),\n",
       " ('reaching', 1),\n",
       " ('stairs', 1),\n",
       " ('waiting', 1),\n",
       " ('dear', 1),\n",
       " ('boy', 1),\n",
       " ('grasp', 1),\n",
       " ('thrust', 1),\n",
       " ('icy', 1),\n",
       " ('offended', 1),\n",
       " ('transparent', 1),\n",
       " ('pointing', 1),\n",
       " ('finger', 1),\n",
       " ('caught', 1),\n",
       " ('sight', 1),\n",
       " ('raised', 1),\n",
       " ('eyebrows', 1),\n",
       " ('proud', 1),\n",
       " ('close', 1),\n",
       " ('recognized', 1),\n",
       " ('tone', 1),\n",
       " ('percy', 1),\n",
       " ('brother', 1),\n",
       " ('kneeling', 1),\n",
       " ('stared', 1),\n",
       " ('laugh', 1),\n",
       " ('etched', 1),\n",
       " ('chapter', 1),\n",
       " ('thirty', 1),\n",
       " ('elder', 1),\n",
       " ('world', 1),\n",
       " ('ended', 1),\n",
       " ('battle', 1),\n",
       " ('twig', 1),\n",
       " ('strewn', 1),\n",
       " ('ground', 1),\n",
       " ('marked', 1),\n",
       " ('outer', 1),\n",
       " ('edge', 1),\n",
       " ('forest', 1),\n",
       " ('opened', 1),\n",
       " ('truly', 1),\n",
       " ('flesh', 1),\n",
       " ('resembled', 1),\n",
       " ('closely', 1),\n",
       " ('escaped', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(item, value) for item, value in contexts[\"ghost\"].items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 11682),\n",
       " ('t', 5489),\n",
       " ('ron', 5375),\n",
       " ('hermione', 4301),\n",
       " ('dumbledore', 2497),\n",
       " ('looked', 2085),\n",
       " ('did', 1792),\n",
       " ('know', 1717),\n",
       " ('just', 1654),\n",
       " ('like', 1516),\n",
       " ('hagrid', 1512),\n",
       " ('got', 1453),\n",
       " ('looking', 1418),\n",
       " ('ve', 1394),\n",
       " ('d', 1391),\n",
       " ('don', 1348),\n",
       " ('snape', 1331),\n",
       " ('eyes', 1311),\n",
       " ('potter', 1292),\n",
       " ('right', 1278),\n",
       " ('time', 1277),\n",
       " ('think', 1261),\n",
       " ('face', 1230),\n",
       " ('wand', 1204),\n",
       " ('look', 1134),\n",
       " ('professor', 1105),\n",
       " ('didn', 1087),\n",
       " ('going', 1084),\n",
       " ('saw', 1079),\n",
       " ('voice', 1070),\n",
       " ('come', 1067),\n",
       " ('door', 1042),\n",
       " ('thought', 1022),\n",
       " ('malfoy', 1010),\n",
       " ('m', 993),\n",
       " ('head', 971),\n",
       " ('room', 956),\n",
       " ('weasley', 953),\n",
       " ('asked', 948),\n",
       " ('hand', 941),\n",
       " ('sirius', 918),\n",
       " ('voldemort', 908),\n",
       " ('himself', 884),\n",
       " ('knew', 877),\n",
       " ('turned', 867),\n",
       " ('away', 819),\n",
       " ('felt', 815),\n",
       " ('way', 808),\n",
       " ('told', 786),\n",
       " ('good', 785),\n",
       " ('heard', 752),\n",
       " ('tell', 740),\n",
       " ('want', 736),\n",
       " ('mr', 728),\n",
       " ('let', 728),\n",
       " ('left', 721),\n",
       " ('moment', 718),\n",
       " ('yeah', 702),\n",
       " ('oh', 673),\n",
       " ('dark', 669),\n",
       " ('lupin', 631),\n",
       " ('really', 630),\n",
       " ('black', 626),\n",
       " ('long', 615),\n",
       " ('little', 609),\n",
       " ('say', 603),\n",
       " ('ginny', 583),\n",
       " ('great', 573),\n",
       " ('neville', 571),\n",
       " ('feet', 558),\n",
       " ('sure', 554),\n",
       " ('people', 553),\n",
       " ('dobby', 539),\n",
       " ('trying', 537),\n",
       " ('fred', 534),\n",
       " ('took', 517),\n",
       " ('inside', 515),\n",
       " ('table', 512),\n",
       " ('came', 510),\n",
       " ('hogwarts', 509),\n",
       " ('mrs', 507),\n",
       " ('went', 506),\n",
       " ('open', 502),\n",
       " ('george', 489),\n",
       " ('pulled', 483),\n",
       " ('uncle', 480),\n",
       " ('death', 472),\n",
       " ('cloak', 469),\n",
       " ('gave', 468),\n",
       " ('make', 468),\n",
       " ('seen', 466),\n",
       " ('hear', 466),\n",
       " ('floor', 464),\n",
       " ('hands', 455),\n",
       " ('thing', 450),\n",
       " ('wanted', 447),\n",
       " ('stood', 446),\n",
       " ('vernon', 441),\n",
       " ('help', 430),\n",
       " ('house', 429),\n",
       " ('night', 426),\n",
       " ('couldn', 425),\n",
       " ('staring', 420),\n",
       " ('gryffindor', 420),\n",
       " ('course', 418),\n",
       " ('hall', 418),\n",
       " ('need', 416),\n",
       " ('whispered', 410),\n",
       " ('mcgonagall', 409),\n",
       " ('happened', 406),\n",
       " ('wasn', 405),\n",
       " ('place', 404),\n",
       " ('quickly', 400),\n",
       " ('doing', 398),\n",
       " ('large', 396),\n",
       " ('bed', 394),\n",
       " ('stared', 393),\n",
       " ('old', 383),\n",
       " ('past', 383),\n",
       " ('sat', 381),\n",
       " ('suddenly', 375),\n",
       " ('school', 375),\n",
       " ('mean', 374),\n",
       " ('quite', 373),\n",
       " ('feeling', 371),\n",
       " ('hard', 371),\n",
       " ('better', 369),\n",
       " ('walked', 369),\n",
       " ('dudley', 369),\n",
       " ('sir', 368),\n",
       " ('boy', 367),\n",
       " ('mind', 363),\n",
       " ('half', 362),\n",
       " ('moody', 360),\n",
       " ('gone', 353),\n",
       " ('second', 350),\n",
       " ('mouth', 348),\n",
       " ('raised', 346),\n",
       " ('arm', 344),\n",
       " ('end', 344),\n",
       " ('year', 342),\n",
       " ('hair', 341),\n",
       " ('talking', 340),\n",
       " ('air', 338),\n",
       " ('eye', 338),\n",
       " ('cedric', 338),\n",
       " ('slightly', 335),\n",
       " ('small', 334),\n",
       " ('bit', 334),\n",
       " ('set', 333),\n",
       " ('won', 332),\n",
       " ('forward', 332),\n",
       " ('window', 326),\n",
       " ('tried', 325),\n",
       " ('quietly', 324),\n",
       " ('started', 322),\n",
       " ('shouted', 321),\n",
       " ('magic', 320),\n",
       " ('robes', 320),\n",
       " ('standing', 319),\n",
       " ('slughorn', 319),\n",
       " ('sitting', 315),\n",
       " ('began', 314),\n",
       " ('man', 314),\n",
       " ('fell', 313),\n",
       " ('stop', 312),\n",
       " ('opened', 311),\n",
       " ('umbridge', 311),\n",
       " ('watching', 310),\n",
       " ('scar', 307),\n",
       " ('yeh', 305),\n",
       " ('day', 302),\n",
       " ('reached', 302),\n",
       " ('light', 301),\n",
       " ('quidditch', 300),\n",
       " ('coming', 297),\n",
       " ('shoulder', 297),\n",
       " ('muttered', 296),\n",
       " ('office', 296),\n",
       " ('feel', 295),\n",
       " ('years', 292),\n",
       " ('sight', 289),\n",
       " ('thinking', 287),\n",
       " ('er', 287),\n",
       " ('okay', 281),\n",
       " ('later', 280),\n",
       " ('stone', 279),\n",
       " ('ministry', 278),\n",
       " ('things', 277),\n",
       " ('slowly', 277),\n",
       " ('luna', 277),\n",
       " ('stopped', 276),\n",
       " ('word', 274),\n",
       " ('wizard', 274),\n",
       " ('idea', 271),\n",
       " ('caught', 267),\n",
       " ('sorry', 267),\n",
       " ('called', 266),\n",
       " ('ask', 265),\n",
       " ('holding', 265),\n",
       " ('wall', 263),\n",
       " ('moved', 263),\n",
       " ('yelled', 262),\n",
       " ('able', 261),\n",
       " ('dead', 258),\n",
       " ('watched', 255),\n",
       " ('far', 255),\n",
       " ('close', 252),\n",
       " ('percy', 252),\n",
       " ('aunt', 251),\n",
       " ('passed', 249),\n",
       " ('wouldn', 248),\n",
       " ('ground', 248),\n",
       " ('followed', 248),\n",
       " ('castle', 248),\n",
       " ('heart', 246),\n",
       " ('hadn', 246),\n",
       " ('leave', 245),\n",
       " ('kept', 245),\n",
       " ('white', 245),\n",
       " ('saying', 244),\n",
       " ('supposed', 244),\n",
       " ('fudge', 243),\n",
       " ('getting', 240),\n",
       " ('rest', 240),\n",
       " ('father', 238),\n",
       " ('read', 238),\n",
       " ('kill', 237),\n",
       " ('believe', 237),\n",
       " ('silence', 237),\n",
       " ('isn', 236),\n",
       " ('pointing', 236),\n",
       " ('understand', 235),\n",
       " ('closed', 235),\n",
       " ('new', 235),\n",
       " ('making', 235),\n",
       " ('outside', 234),\n",
       " ('words', 233),\n",
       " ('best', 232),\n",
       " ('shaking', 229),\n",
       " ('slytherin', 229),\n",
       " ('krum', 228),\n",
       " ('minutes', 227),\n",
       " ('taking', 225),\n",
       " ('haven', 225),\n",
       " ('life', 224),\n",
       " ('talk', 224),\n",
       " ('shut', 222),\n",
       " ('desk', 222),\n",
       " ('dursleys', 221),\n",
       " ('fast', 221),\n",
       " ('loudly', 221),\n",
       " ('riddle', 221),\n",
       " ('red', 219),\n",
       " ('does', 219),\n",
       " ('hit', 217),\n",
       " ('work', 217),\n",
       " ('blood', 216),\n",
       " ('corner', 216),\n",
       " ('nose', 216),\n",
       " ('try', 216),\n",
       " ('cho', 216),\n",
       " ('held', 215),\n",
       " ('book', 215),\n",
       " ('hurried', 215),\n",
       " ('shook', 214),\n",
       " ('pain', 214),\n",
       " ('elf', 214),\n",
       " ('nearly', 213),\n",
       " ('arms', 212),\n",
       " ('common', 211),\n",
       " ('chair', 211),\n",
       " ('sound', 210),\n",
       " ('lot', 210),\n",
       " ('corridor', 210),\n",
       " ('speak', 209),\n",
       " ('darkness', 207),\n",
       " ('mother', 206),\n",
       " ('cold', 206),\n",
       " ('morning', 205),\n",
       " ('letter', 204),\n",
       " ('taken', 204),\n",
       " ('use', 203),\n",
       " ('exactly', 202),\n",
       " ('dear', 202),\n",
       " ('noticed', 202),\n",
       " ('high', 200),\n",
       " ('kreacher', 200),\n",
       " ('remember', 199),\n",
       " ('class', 199),\n",
       " ('crowd', 199),\n",
       " ('silver', 199),\n",
       " ('given', 198),\n",
       " ('water', 197),\n",
       " ('lockhart', 197),\n",
       " ('madam', 196),\n",
       " ('hedwig', 196),\n",
       " ('eaters', 196),\n",
       " ('potion', 195),\n",
       " ('having', 195),\n",
       " ('lord', 194),\n",
       " ('moving', 194),\n",
       " ('seized', 193),\n",
       " ('invisibility', 193),\n",
       " ('owl', 192),\n",
       " ('chapter', 191),\n",
       " ('appeared', 191),\n",
       " ('crouch', 191),\n",
       " ('stay', 190),\n",
       " ('wood', 190),\n",
       " ('lost', 189),\n",
       " ('filch', 189),\n",
       " ('smile', 188),\n",
       " ('fingers', 188),\n",
       " ('glanced', 188),\n",
       " ('wearing', 188),\n",
       " ('nodded', 186),\n",
       " ('point', 185),\n",
       " ('answer', 185),\n",
       " ('realized', 184),\n",
       " ('wait', 184),\n",
       " ('added', 184),\n",
       " ('stairs', 183),\n",
       " ('cup', 183),\n",
       " ('maybe', 182),\n",
       " ('pointed', 182),\n",
       " ('dad', 181),\n",
       " ('stand', 180),\n",
       " ('waiting', 180),\n",
       " ('spoke', 179),\n",
       " ('telling', 179),\n",
       " ('thanks', 178),\n",
       " ('dementors', 178),\n",
       " ('ter', 177),\n",
       " ('foot', 177),\n",
       " ('petunia', 176),\n",
       " ('straight', 176),\n",
       " ('low', 174),\n",
       " ('students', 174),\n",
       " ('bag', 172),\n",
       " ('turning', 172),\n",
       " ('entrance', 172),\n",
       " ('ears', 172),\n",
       " ('team', 172),\n",
       " ('threw', 171),\n",
       " ('lay', 170),\n",
       " ('wrong', 170),\n",
       " ('steps', 170),\n",
       " ('magical', 170),\n",
       " ('watch', 169),\n",
       " ('dropped', 169),\n",
       " ('sort', 169),\n",
       " ('free', 169),\n",
       " ('forest', 169),\n",
       " ('continued', 169),\n",
       " ('used', 168),\n",
       " ('breath', 168),\n",
       " ('kitchen', 167),\n",
       " ('parents', 167),\n",
       " ('tonks', 167),\n",
       " ('usual', 166),\n",
       " ('matter', 166),\n",
       " ('green', 165),\n",
       " ('met', 165),\n",
       " ('body', 165),\n",
       " ('run', 164),\n",
       " ('leaving', 164),\n",
       " ('parchment', 164),\n",
       " ('loud', 164),\n",
       " ('gold', 163),\n",
       " ('bagman', 163),\n",
       " ('bad', 162),\n",
       " ('died', 162),\n",
       " ('listen', 162),\n",
       " ('ready', 162),\n",
       " ('pulling', 161),\n",
       " ('staircase', 161),\n",
       " ('lying', 160),\n",
       " ('wizards', 160),\n",
       " ('broom', 160),\n",
       " ('fine', 160),\n",
       " ('deep', 159),\n",
       " ('fact', 159),\n",
       " ('expression', 159),\n",
       " ('goyle', 158),\n",
       " ('trunk', 157),\n",
       " ('dean', 157),\n",
       " ('fleur', 157),\n",
       " ('entered', 156),\n",
       " ('ran', 155),\n",
       " ('trelawney', 155),\n",
       " ('doesn', 154),\n",
       " ('finished', 152),\n",
       " ('crabbe', 152),\n",
       " ('world', 151),\n",
       " ('glass', 151),\n",
       " ('seconds', 151),\n",
       " ('care', 151),\n",
       " ('sword', 151),\n",
       " ('noise', 150),\n",
       " ('start', 150),\n",
       " ('mad', 150),\n",
       " ('pocket', 150),\n",
       " ('girl', 150),\n",
       " ('golden', 150),\n",
       " ('flew', 149),\n",
       " ('chest', 149),\n",
       " ('legs', 149),\n",
       " ('snake', 148),\n",
       " ('hold', 148),\n",
       " ('friends', 148),\n",
       " ('sit', 148),\n",
       " ('angry', 147),\n",
       " ('instead', 147),\n",
       " ('shall', 147),\n",
       " ('returned', 147),\n",
       " ('true', 146),\n",
       " ('seat', 146),\n",
       " ('hat', 146),\n",
       " ('hope', 146),\n",
       " ('repeated', 146),\n",
       " ('snitch', 146),\n",
       " ('soon', 145),\n",
       " ('turn', 145),\n",
       " ('beneath', 145),\n",
       " ('tiny', 144),\n",
       " ('days', 144),\n",
       " ('chance', 142),\n",
       " ('griphook', 142),\n",
       " ('draco', 142),\n",
       " ('memory', 142),\n",
       " ('books', 141),\n",
       " ('friend', 141),\n",
       " ('closer', 141),\n",
       " ('o', 141),\n",
       " ('happy', 140),\n",
       " ('spell', 140),\n",
       " ('train', 140),\n",
       " ('burst', 140),\n",
       " ('stepped', 139),\n",
       " ('glasses', 139),\n",
       " ('sign', 139),\n",
       " ('aside', 139),\n",
       " ('sent', 139),\n",
       " ('scrimgeour', 139),\n",
       " ('enormous', 138),\n",
       " ('roared', 137),\n",
       " ('pushed', 137),\n",
       " ('real', 137),\n",
       " ('map', 136),\n",
       " ('car', 136),\n",
       " ('wands', 136),\n",
       " ('match', 136),\n",
       " ('eater', 136),\n",
       " ('running', 135),\n",
       " ('finally', 134),\n",
       " ('neck', 134),\n",
       " ('thank', 134),\n",
       " ('seamus', 134),\n",
       " ('knees', 133),\n",
       " ('case', 133),\n",
       " ('doors', 133),\n",
       " ('vanished', 133),\n",
       " ('odd', 133),\n",
       " ('ear', 133),\n",
       " ('completely', 133),\n",
       " ('curse', 133),\n",
       " ('looks', 132),\n",
       " ('james', 131),\n",
       " ('forehead', 131),\n",
       " ('arrived', 131),\n",
       " ('listening', 131),\n",
       " ('flying', 131),\n",
       " ('evening', 131),\n",
       " ('meant', 131),\n",
       " ('christmas', 130),\n",
       " ('heads', 130),\n",
       " ('grounds', 130),\n",
       " ('muggle', 130),\n",
       " ('person', 130),\n",
       " ('cauldron', 130),\n",
       " ('suppose', 129),\n",
       " ('smiling', 129),\n",
       " ('tears', 129),\n",
       " ('question', 129),\n",
       " ('jumped', 129),\n",
       " ('stomach', 128),\n",
       " ('wondered', 128),\n",
       " ('tightly', 128),\n",
       " ('grinning', 127),\n",
       " ('potions', 127),\n",
       " ('hurt', 127),\n",
       " ('hogsmeade', 127),\n",
       " ('family', 126),\n",
       " ('walking', 126),\n",
       " ('hour', 126),\n",
       " ('trouble', 126),\n",
       " ('wide', 126),\n",
       " ('backward', 126),\n",
       " ('barely', 126),\n",
       " ('firebolt', 126),\n",
       " ('laugh', 125),\n",
       " ('probably', 125),\n",
       " ('angrily', 125),\n",
       " ('kind', 125),\n",
       " ('walk', 124),\n",
       " ('managed', 124),\n",
       " ('pleased', 124),\n",
       " ('home', 124),\n",
       " ('crookshanks', 124),\n",
       " ('order', 123),\n",
       " ('known', 123),\n",
       " ('worse', 123),\n",
       " ('aren', 123),\n",
       " ('mirror', 123),\n",
       " ('ceiling', 122),\n",
       " ('giving', 122),\n",
       " ('dog', 122),\n",
       " ('stupid', 122),\n",
       " ('ago', 122),\n",
       " ('wormtail', 122),\n",
       " ('laughed', 121),\n",
       " ('mum', 121),\n",
       " ('smiled', 121),\n",
       " ('witch', 121),\n",
       " ('longer', 121),\n",
       " ('times', 120),\n",
       " ('wondering', 120),\n",
       " ('sleep', 120),\n",
       " ('reckon', 120),\n",
       " ('meet', 120),\n",
       " ('near', 120),\n",
       " ('tower', 120),\n",
       " ('slid', 119),\n",
       " ('forced', 119),\n",
       " ('killed', 119),\n",
       " ('clear', 119),\n",
       " ('quirrell', 119),\n",
       " ('late', 119),\n",
       " ('ah', 118),\n",
       " ('important', 117),\n",
       " ('pair', 117),\n",
       " ('leg', 117),\n",
       " ('pale', 117),\n",
       " ('tent', 117),\n",
       " ('goblin', 116),\n",
       " ('funny', 115),\n",
       " ('led', 115),\n",
       " ('ahead', 115),\n",
       " ('son', 114),\n",
       " ('direction', 114),\n",
       " ('horrible', 114),\n",
       " ('stuff', 114),\n",
       " ('master', 114),\n",
       " ('rita', 114),\n",
       " ('broke', 113),\n",
       " ('minute', 113),\n",
       " ('apart', 113),\n",
       " ('screamed', 113),\n",
       " ('brought', 113),\n",
       " ('hidden', 113),\n",
       " ('starting', 112),\n",
       " ('sounded', 112),\n",
       " ('trees', 112),\n",
       " ('bent', 111),\n",
       " ('silent', 111),\n",
       " ('catch', 111),\n",
       " ('surprise', 111),\n",
       " ('summer', 111),\n",
       " ('picked', 111),\n",
       " ('spot', 111),\n",
       " ('slipped', 111),\n",
       " ('quick', 111),\n",
       " ('girls', 111),\n",
       " ('dormitory', 111),\n",
       " ('broken', 110),\n",
       " ('shot', 110),\n",
       " ('climbed', 110),\n",
       " ('buckbeak', 110),\n",
       " ('happen', 109),\n",
       " ('arts', 109),\n",
       " ('points', 109),\n",
       " ('lessons', 109),\n",
       " ('knows', 108),\n",
       " ('somebody', 108),\n",
       " ('seven', 108),\n",
       " ('clock', 108),\n",
       " ('tonight', 108),\n",
       " ('laughing', 108),\n",
       " ('footsteps', 108),\n",
       " ('simply', 107),\n",
       " ('tea', 107),\n",
       " ('edge', 107),\n",
       " ('expected', 107),\n",
       " ('opposite', 107),\n",
       " ('remembered', 107),\n",
       " ('merely', 107),\n",
       " ('apparently', 107),\n",
       " ('leapt', 106),\n",
       " ('hardly', 106),\n",
       " ('breakfast', 106),\n",
       " ('throat', 106),\n",
       " ('clearly', 106),\n",
       " ('dunno', 106),\n",
       " ('hole', 106),\n",
       " ('different', 106),\n",
       " ('short', 106),\n",
       " ('fear', 105),\n",
       " ('filled', 105),\n",
       " ('truth', 105),\n",
       " ('actually', 105),\n",
       " ('headmaster', 105),\n",
       " ('recognized', 105),\n",
       " ('dinner', 105),\n",
       " ('pomfrey', 105),\n",
       " ('snapped', 104),\n",
       " ('gasped', 104),\n",
       " ('dangerous', 104),\n",
       " ('faces', 104),\n",
       " ('dragon', 104),\n",
       " ('sudden', 104),\n",
       " ('lavender', 104),\n",
       " ('worried', 104),\n",
       " ('speaking', 104),\n",
       " ('mundungus', 104),\n",
       " ('goblet', 103),\n",
       " ('forgotten', 103),\n",
       " ('round', 103),\n",
       " ('big', 103),\n",
       " ('nice', 103),\n",
       " ('wants', 103),\n",
       " ('tight', 103),\n",
       " ('rose', 103),\n",
       " ('carefully', 103),\n",
       " ('portrait', 103),\n",
       " ('remained', 103),\n",
       " ('firmly', 102),\n",
       " ('week', 102),\n",
       " ('disappeared', 102),\n",
       " ('hello', 102),\n",
       " ('wish', 102),\n",
       " ('horcrux', 102),\n",
       " ('sky', 101),\n",
       " ('heavy', 101),\n",
       " ('couple', 101),\n",
       " ('woman', 101),\n",
       " ('drew', 100),\n",
       " ('story', 100),\n",
       " ('prophet', 100),\n",
       " ('cried', 100),\n",
       " ('seeing', 100),\n",
       " ('teacher', 100),\n",
       " ('break', 100),\n",
       " ('bye', 99),\n",
       " ('quiet', 99),\n",
       " ('possible', 99),\n",
       " ('nervously', 99),\n",
       " ('return', 99),\n",
       " ('lesson', 99),\n",
       " ('walls', 99),\n",
       " ('bellowed', 99),\n",
       " ('prophecy', 99),\n",
       " ('surprised', 98),\n",
       " ('sank', 98),\n",
       " ('middle', 98),\n",
       " ('tree', 98),\n",
       " ('hastily', 98),\n",
       " ('handed', 97),\n",
       " ('bedroom', 97),\n",
       " ('says', 97),\n",
       " ('charm', 97),\n",
       " ('miss', 97),\n",
       " ('picture', 97),\n",
       " ('covered', 97),\n",
       " ('shock', 97),\n",
       " ('blue', 97),\n",
       " ('defense', 97),\n",
       " ('skeeter', 97),\n",
       " ('luck', 96),\n",
       " ('boys', 96),\n",
       " ('huge', 96),\n",
       " ('upstairs', 96),\n",
       " ('hot', 96),\n",
       " ('beaming', 96),\n",
       " ('lake', 96),\n",
       " ('strange', 95),\n",
       " ('waited', 95),\n",
       " ('teeth', 95),\n",
       " ('herself', 95),\n",
       " ('hours', 95),\n",
       " ('wing', 95),\n",
       " ('stan', 95),\n",
       " ('karkaroff', 95),\n",
       " ('glad', 94),\n",
       " ('fighting', 94),\n",
       " ('piece', 94),\n",
       " ('sideways', 94),\n",
       " ('strode', 94),\n",
       " ('swung', 94),\n",
       " ('albus', 94),\n",
       " ('laughter', 94),\n",
       " ('figure', 94),\n",
       " ('task', 94),\n",
       " ('terrible', 93),\n",
       " ('knocked', 93),\n",
       " ('note', 93),\n",
       " ('scared', 93),\n",
       " ('windows', 93),\n",
       " ('leaned', 93),\n",
       " ('ball', 93),\n",
       " ('hoping', 93),\n",
       " ('hesitated', 93),\n",
       " ('fight', 92),\n",
       " ('attack', 92),\n",
       " ('thoughts', 92),\n",
       " ('gazing', 92),\n",
       " ('hi', 92),\n",
       " ('number', 91),\n",
       " ('expect', 91),\n",
       " ('bright', 91),\n",
       " ('difficult', 91),\n",
       " ('ought', 91),\n",
       " ('decided', 91),\n",
       " ('giant', 91),\n",
       " ('fixed', 91),\n",
       " ('homework', 91),\n",
       " ('fang', 91),\n",
       " ('happily', 91),\n",
       " ('closely', 91),\n",
       " ('bellatrix', 91),\n",
       " ('asleep', 90),\n",
       " ('box', 90),\n",
       " ('normal', 90),\n",
       " ('hospital', 90),\n",
       " ('carrying', 90),\n",
       " ('approached', 90),\n",
       " ('cut', 89),\n",
       " ('wake', 89),\n",
       " ('eyebrows', 89),\n",
       " ('job', 89),\n",
       " ('shouldn', 89),\n",
       " ('saved', 89),\n",
       " ('bludger', 89),\n",
       " ('split', 89),\n",
       " ('pettigrew', 89),\n",
       " ('living', 88),\n",
       " ('needed', 88),\n",
       " ('tail', 88),\n",
       " ('page', 88),\n",
       " ('nervous', 88),\n",
       " ('missed', 88),\n",
       " ('hasn', 88),\n",
       " ('weasleys', 88),\n",
       " ('follow', 88),\n",
       " ('joined', 88),\n",
       " ('understood', 88),\n",
       " ('phoenix', 87),\n",
       " ('crossed', 87),\n",
       " ('quill', 87),\n",
       " ('bottle', 87),\n",
       " ('ollivander', 87),\n",
       " ('extremely', 87),\n",
       " ('colin', 87),\n",
       " ('azkaban', 86),\n",
       " ('secret', 86),\n",
       " ('hurry', 86),\n",
       " ('allowed', 86),\n",
       " ('paper', 86),\n",
       " ('eagerly', 86),\n",
       " ('landed', 86),\n",
       " ('subject', 86),\n",
       " ('library', 86),\n",
       " ('reason', 86),\n",
       " ('headed', 86),\n",
       " ('attention', 86),\n",
       " ('nick', 86),\n",
       " ('diary', 86),\n",
       " ('young', 85),\n",
       " ('grabbed', 85),\n",
       " ('hanging', 85),\n",
       " ('ravenclaw', 85),\n",
       " ('following', 85),\n",
       " ('step', 84),\n",
       " ('egg', 84),\n",
       " ('news', 84),\n",
       " ('furiously', 84),\n",
       " ('weren', 84),\n",
       " ('problem', 84),\n",
       " ('spent', 84),\n",
       " ('shop', 84),\n",
       " ('explain', 84),\n",
       " ('daily', 84),\n",
       " ('voices', 84),\n",
       " ('clutching', 84),\n",
       " ('myrtle', 84),\n",
       " ('diggory', 84),\n",
       " ('rolled', 83),\n",
       " ('visit', 83),\n",
       " ('classroom', 83),\n",
       " ('fallen', 83),\n",
       " ('fat', 82),\n",
       " ('liked', 82),\n",
       " ('fly', 82),\n",
       " ('sharply', 82),\n",
       " ('safe', 82),\n",
       " ('sense', 82),\n",
       " ('helped', 82),\n",
       " ('present', 82),\n",
       " ('faced', 82),\n",
       " ('throwing', 82),\n",
       " ('fifty', 82),\n",
       " ('halfway', 82),\n",
       " ('certainly', 81),\n",
       " ('join', 81),\n",
       " ('worry', 81),\n",
       " ('hurrying', 81),\n",
       " ('die', 81),\n",
       " ('working', 81),\n",
       " ('weeks', 81),\n",
       " ('drink', 81),\n",
       " ('conversation', 81),\n",
       " ('tournament', 81),\n",
       " ('muggles', 80),\n",
       " ('save', 80),\n",
       " ('impatiently', 80),\n",
       " ('seriously', 79),\n",
       " ('putting', 79),\n",
       " ('asking', 79),\n",
       " ('practice', 79),\n",
       " ('plan', 79),\n",
       " ('anymore', 79),\n",
       " ('tom', 79),\n",
       " ('familiar', 79),\n",
       " ('slytherins', 79),\n",
       " ('field', 79),\n",
       " ('angelina', 79),\n",
       " ('surface', 79),\n",
       " ('patronus', 79),\n",
       " ('power', 78),\n",
       " ('street', 78),\n",
       " ('hiding', 78),\n",
       " ('birthday', 78),\n",
       " ('cupboard', 78),\n",
       " ('relief', 78),\n",
       " ('alive', 78),\n",
       " ('terrified', 78),\n",
       " ('anger', 78),\n",
       " ('breathing', 78),\n",
       " ('minister', 78),\n",
       " ('play', 78),\n",
       " ('broomstick', 78),\n",
       " ('softly', 78),\n",
       " ('dungeon', 78),\n",
       " ('view', 78),\n",
       " ('touch', 78),\n",
       " ('frightened', 78),\n",
       " ('ernie', 78),\n",
       " ('winky', 78),\n",
       " ('questions', 77),\n",
       " ('gray', 77),\n",
       " ('spells', 77),\n",
       " ('force', 77),\n",
       " ('compartment', 77),\n",
       " ('granger', 77),\n",
       " ('means', 77),\n",
       " ('bring', 76),\n",
       " ('change', 76),\n",
       " ('reading', 76),\n",
       " ('definitely', 76),\n",
       " ('stuck', 76),\n",
       " ('busy', 76),\n",
       " ('forget', 76),\n",
       " ('locket', 76),\n",
       " ('chamber', 75),\n",
       " ('famous', 75),\n",
       " ('garden', 75),\n",
       " ('meeting', 75),\n",
       " ('skin', 75),\n",
       " ('trembling', 75),\n",
       " ('food', 75),\n",
       " ('obviously', 75),\n",
       " ('nearer', 75),\n",
       " ('fall', 75),\n",
       " ('peering', 75),\n",
       " ('soft', 75),\n",
       " ('moments', 75),\n",
       " ('path', 75),\n",
       " ('lucky', 75),\n",
       " ('keeping', 75),\n",
       " ('immediately', 75),\n",
       " ('dementor', 75),\n",
       " ('shoulders', 74),\n",
       " ('row', 74),\n",
       " ('bet', 74),\n",
       " ('owls', 74),\n",
       " ('peeves', 74),\n",
       " ('dragged', 74),\n",
       " ('today', 73),\n",
       " ('drive', 73),\n",
       " ('thrown', 73),\n",
       " ('teachers', 73),\n",
       " ('wizarding', 73),\n",
       " ('cage', 73),\n",
       " ('muttering', 73),\n",
       " ('stunned', 73),\n",
       " ('parvati', 73),\n",
       " ('copy', 73),\n",
       " ('scarlet', 73),\n",
       " ('creatures', 73),\n",
       " ('arthur', 73),\n",
       " ('fawkes', 73),\n",
       " ('greyback', 73),\n",
       " ('dream', 72),\n",
       " ('demanded', 72),\n",
       " ('eh', 72),\n",
       " ('showed', 72),\n",
       " ('falling', 72),\n",
       " ('detention', 72),\n",
       " ('distant', 72),\n",
       " ('information', 72),\n",
       " ('warning', 71),\n",
       " ('chocolate', 71),\n",
       " ('grinned', 71),\n",
       " ('changed', 71),\n",
       " ('brother', 71),\n",
       " ('term', 71),\n",
       " ('grass', 71),\n",
       " ('pass', 71),\n",
       " ('particularly', 71),\n",
       " ('brain', 71),\n",
       " ('dare', 70),\n",
       " ('ignored', 70),\n",
       " ('sighed', 70),\n",
       " ('growled', 70),\n",
       " ('perfectly', 70),\n",
       " ('marble', 70),\n",
       " ('mark', 70),\n",
       " ('playing', 70),\n",
       " ('love', 70),\n",
       " ('emerged', 70),\n",
       " ('snarled', 69),\n",
       " ('sharp', 69),\n",
       " ('usually', 69),\n",
       " ('tomorrow', 69),\n",
       " ('wonder', 69),\n",
       " ('heavily', 69),\n",
       " ('sped', 69),\n",
       " ('panting', 69),\n",
       " ('spotted', 69),\n",
       " ('inches', 69),\n",
       " ('brightly', 69),\n",
       " ('drawing', 69),\n",
       " ('invisible', 69),\n",
       " ('party', 69),\n",
       " ('w', 69),\n",
       " ('unable', 69),\n",
       " ('aware', 69),\n",
       " ('deserted', 69),\n",
       " ('single', 68),\n",
       " ('sick', 68),\n",
       " ('sister', 68),\n",
       " ('writing', 68),\n",
       " ('shortly', 68),\n",
       " ('earth', 68),\n",
       " ('shocked', 68),\n",
       " ('mention', 68),\n",
       " ('eat', 68),\n",
       " ('line', 68),\n",
       " ('nearest', 68),\n",
       " ('fault', 68),\n",
       " ('whisper', 68),\n",
       " ('lucius', 68),\n",
       " ('swept', 68),\n",
       " ('prince', 67),\n",
       " ('careful', 67),\n",
       " ('passing', 67),\n",
       " ('cast', 67),\n",
       " ('seats', 67),\n",
       " ('halt', 67),\n",
       " ('send', 67),\n",
       " ('panted', 67),\n",
       " ('frowning', 67),\n",
       " ('determined', 67),\n",
       " ('shouting', 67),\n",
       " ('pretty', 67),\n",
       " ('anxiously', 67),\n",
       " ('wind', 66),\n",
       " ('everybody', 66),\n",
       " ('lit', 66),\n",
       " ('group', 66),\n",
       " ('waving', 66),\n",
       " ('upward', 66),\n",
       " ('hissed', 66),\n",
       " ('flung', 66),\n",
       " ('lowered', 66),\n",
       " ('pensieve', 66),\n",
       " ('stretched', 65),\n",
       " ('interested', 65),\n",
       " ('check', 65),\n",
       " ('hufflepuff', 65),\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(item, value) for item, value in contexts[\"harry\"].items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The co-occurrence matrix of a very large corpus should give a meaningful summary of how a word is used in general. A single row of that matrix is already a __word vector__ of size $N$. However such vectors are extremely sparse, and for large corpora the size of $N$ will become unwieldy. We will follow along the paper in designing an algorithm that can compress the word vectors while retaining most of their informational content. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "For the resulting vectors to actually be informative, the source corpus should have a size of at least a few billion words; on the contrary, our corpus enumerates merely a million words, so we can't expect our results to be as great.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity and Stability\n",
    "\n",
    "Our matrix $X$ is very sparse; most of its elements are zero.\n",
    "\n",
    "**Coding 1.1**: Find what the ratio of non-zero elements is.\n",
    "\n",
    "_Hint_: The function `non_zero_ratio` should return a `float` rather than a `FloatTensor`. Remember `.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_zero_ratio(sparse_matrix: LongTensor) -> float:\n",
    "    zeros = np.count_nonzero(sparse_matrix == 0)\n",
    "    nonzeros = np.count_nonzero(sparse_matrix != 0) \n",
    "    return nonzeros / (nonzeros + zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero ratio in the input matrix is 0.108448515107535\n"
     ]
    }
   ],
   "source": [
    "non_zero_ratio(X)\n",
    "type(non_zero_ratio(X))#Nonzero ratio is 0.108 with roughly ~10% nonzeros\n",
    "print('Zero ratio in the input matrix is {}'.format(non_zero_ratio(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will soon need to perform division and find the logarithm of ${X}$. Neither of the two operations are well-defined for $0$. That's why for further processing we want to have a matrix without any zero elements. \n",
    "\n",
    "**Coding 1.2**: Change the matrix's datatype to a `torch.float` and add a small constant to it (e.g. $0.1$) to ensure numerical stability while maintaining sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        ...,\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000]])\n"
     ]
    }
   ],
   "source": [
    "X = X.to(torch.float) + 0.1\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Show the completed code to your teacher before proceeding</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From co-occurrence counts to probabilities\n",
    "From the paper: \n",
    "> Let the matrix of word-word co-occurrence counts be denoted by $X$, whose entries $X_{ij}$ tabulate the number of times word $j$ occurs in the context of word $i$.  Let $X_i$= $\\sum_{k} X_{ik}$ be the number of times any word appears in the context of word $i$. Finally, let $P_{ij} = P(j  | i) =  X_{ij}/X_i$ be the probability that word $j$ appear in the context of word $i$. \n",
    "\n",
    "**Coding 2**: Complete the function `to_probabilities` that accepts a co-occurrence matrix and returns the probability matrix $P$. \n",
    "\n",
    "_Hint_: Remember broadcasting and `torch.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_probabilities(count_matrix: FloatTensor) -> FloatTensor:\n",
    "    return count_matrix/torch.sum(count_matrix, dim = 1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2439e-04, 1.2439e-04, 1.2439e-04,  ..., 1.2439e-04, 1.2439e-04,\n",
      "         1.2439e-04],\n",
      "        [4.5067e-05, 4.5067e-05, 4.5067e-05,  ..., 4.5067e-05, 4.5067e-05,\n",
      "         4.5067e-05],\n",
      "        [9.7191e-05, 9.7191e-05, 9.7191e-05,  ..., 9.7191e-05, 9.7191e-05,\n",
      "         9.7191e-05],\n",
      "        ...,\n",
      "        [1.3177e-04, 1.3177e-04, 1.3177e-04,  ..., 1.3177e-04, 1.3177e-04,\n",
      "         1.3177e-04],\n",
      "        [1.1602e-04, 1.1602e-04, 1.1602e-04,  ..., 1.1602e-04, 1.1602e-04,\n",
      "         1.1602e-04],\n",
      "        [7.0328e-05, 7.0328e-05, 7.0328e-05,  ..., 7.0328e-05, 7.0328e-05,\n",
      "         7.0328e-05]])\n"
     ]
    }
   ],
   "source": [
    "P = to_probabilities(X)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Show the completed code to your teacher before proceeding</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probing words\n",
    "\n",
    "From the paper:\n",
    "> Consider two words $i$ and $j$ that exhibit a particular aspect of interest. The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, $k$.  For words $k$ related to $i$ but not $j$, we expect the ratio $P_{ik}/P_{jk}$ will be large.  Similarly, for words $k$ related to $j$ but not $i$, the ratio should be small. For words $k$ that are either related to both $i$ and $j$, or to neither, the ratio should be close to one.\n",
    "\n",
    "**Coding 3.1**: Complete the function `query` that accepts two words $w_i$ and $w_j$, a vocab $V$ and a probability matrix ${P}$, maps each word to its corresponding index and returns the probability $P(j  |  i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(word_i: str, word_j: str, vocab: Dict[str, int], probability_matrix: FloatTensor) -> float:  \n",
    "    i = vocab[word_i]\n",
    "    j = vocab[word_j]\n",
    "    \n",
    "    return probability_matrix[i][j].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding 3.2**: Then, complete the function `probe` that accepts three words $w_i$, $w_j$ and $w_k$, a vocab $V$ and a probability matrix ${P}$, calls `query` and returns the ratio $P(k |  i) / P(k  |  j)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe(word_i: str, word_j: str, word_k: str, vocab: Dict[str, int], probability_matrix: FloatTensor) -> float:\n",
    "    Pik = query(word_i, word_k, vocab, probability_matrix)\n",
    "    Pjk = query(word_j, word_k, vocab, probability_matrix)\n",
    "    \n",
    "    return Pik / Pjk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Show the completed code to your teacher before proceeding</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's probe a few words and examine whether the authors' claim holds even for our (tiny) corpus. Feel free to add your own word triplets and experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tea wand spell 0.42263388565501575\n",
      "tea wand cup 24.61305850905283\n",
      "\n",
      "voldemort hagrid curse 8.710822275728288\n",
      "voldemort hagrid beast 0.5588329163940012\n",
      "\n",
      "mcgonagall snape potions 0.0036196233324942884\n",
      "mcgonagall snape transfiguration 43.71879996208764\n",
      "\n",
      "hedwig scabbers owl 5.626579680798838\n",
      "hedwig scabbers rat 0.017819204280628394\n",
      "\n",
      "ron hermione book 0.6783071504063404\n",
      "ron hermione red 2.0382548360401223\n",
      "\n",
      "ron hermione 2015 0.8539477180887408\n",
      "ron hermione gryffindor 1.1493178255355085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"tea\", \"wand\", \"spell\", probe(\"tea\", \"wand\", \"spell\", vocab, P))\n",
    "print(\"tea\", \"wand\", \"cup\", probe(\"tea\", \"wand\", \"cup\", vocab, P))\n",
    "print()\n",
    "\n",
    "print(\"voldemort\", \"hagrid\", \"curse\", probe(\"voldemort\", \"hagrid\", \"curse\", vocab, P))\n",
    "print(\"voldemort\", \"hagrid\", \"beast\", probe(\"voldemort\", \"hagrid\", \"beast\", vocab, P))\n",
    "print()\n",
    "\n",
    "print(\"mcgonagall\", \"snape\", \"potions\", probe(\"mcgonagall\", \"snape\", \"potions\", vocab, P))\n",
    "print(\"mcgonagall\", \"snape\", \"transfiguration\", probe(\"mcgonagall\", \"snape\", \"transfiguration\", vocab, P))\n",
    "print()\n",
    "\n",
    "print(\"hedwig\", \"scabbers\", \"owl\", probe(\"hedwig\", \"scabbers\", \"owl\", vocab, P))\n",
    "print(\"hedwig\", \"scabbers\", \"rat\", probe(\"hedwig\", \"scabbers\", \"rat\", vocab, P))\n",
    "print()\n",
    "\n",
    "print(\"ron\", \"hermione\", \"book\", probe(\"ron\", \"hermione\", \"book\", vocab, P))\n",
    "print(\"ron\", \"hermione\", \"red\", probe(\"ron\", \"hermione\", \"red\", vocab, P))\n",
    "print()\n",
    "\n",
    "print(\"ron\", \"hermione\", \"2015\", probe(\"ron\", \"hermione\", \"2015\", vocab, P))\n",
    "print(\"ron\", \"hermione\", \"gryffindor\", probe(\"ron\", \"hermione\", \"gryffindor\", vocab, P))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation 1**: Give a brief interpretation of the results you got. Do they correspond to your expectations? Why or why not?\n",
    "\n",
    "*Hint*: When do we expect the ratio value to be high, low or close to 1? Refer to the GloVe paper for guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1**: For a word **k** related to **i** but not **j**, $\\frac{P_{ik}}{P_{jk}}$ will be large. In the code above $\\frac{P(tea, cup)}{P(wand, cup)}$ is 2.639, because the nominator is very large (many coocurances for 'tea' and 'cup'), while the denominator is small (few coocurances for 'wand' and 'cup'). Similarly, for words **k** related to **j** but not **i** the ratio should be small; here $\\frac{P(tea, spell)}{P(wand, spell)}$ is 0.0476, becasue the nominator is small (few or no coocurances for 'tea' and 'spell'), while the denominator is very large.\n",
    "\n",
    "\n",
    "For words **k** like 'gryffindor' or '2015', that are either related to both **i** (ron) and **j** (hermione), or to neither, the ratio should be close to one (which is the case as shown by the ratio's 1 and 1.34)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen if we tried probing out-of-domain words? Use the words that the authors report in the paper as discriminative for \"ice\" and \"steam\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ice steam solid 0.08204092670683832\n",
      "ice steam gas 0.9024501648386338\n"
     ]
    }
   ],
   "source": [
    "word1 = \"solid\"\n",
    "word2 = \"gas\"\n",
    "print(\"ice\", \"steam\", word1, probe(\"ice\", \"steam\", word1, vocab, P))\n",
    "print(\"ice\", \"steam\", word2, probe(\"ice\", \"steam\", word2, vocab, P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation 2**: Give an interpretation of the results you got. Do they match what the authors report in the paper? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:**: The authors of the GloVe paper find a large ratio for ice, steam, solid and a low ratio for ice, steam, gas. This is however not the case for this corpus. The coocurance of words depend on the theme of the corpus, hence probing out-of-domain words (such as the thermodynamics example) does not return the same results as the authors because they probed a completely different corpus. This might eb due to 2 reasons:\n",
    "\n",
    "* **Corpus context**: The theme of a text determines the context of a word with respect to other words. In a textbook about thermodynamics *ice* would co-occur more often with *solid* due to the context it is written in - laws of physics, while in a book about nature, *ice* would occur more often in the context of boreal or alpine landscapes and the word *solid*  would probaly be rarely mentioned, if at all, since it would not really fit the theme of the book (and when occuring it would occur in a different context, such as a *solid* rock). \n",
    "* **Word frequency**: These out-of-domain examples are probably few in general for the HP corpus. Therefore they do not occur in the expected context very often, which leads to a ratio closer to one. This small sample size might result in a sample error: there are far to few instances of the words present in the test and rather coocur by chance with other words (far-related, or non-related words) To get somewhat more sensible results more instances  of those words are necessary. \n",
    "\n",
    "Concluding, similar results would be more likely if J.K. Rowling had written about a class about \"the defense against the dark, but still very much according to the laws of physics, thermodynamics\" instead of \"the defense against the dark arts classes\". Steam would in that case probably co-occur more often with gas than with solid. Or if the corpus would be extended with books about the states of matter. The co-occurrences between ice and steam would then probably be more like the examples from the authors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Vectors\n",
    "\n",
    "Now, we would like to convert these long sparse vectors into short dense ones. \n",
    "\n",
    "The conversion should be such that the probability ratios we inspected earlier may still be reconstructed via some (for now, unknown) operation $F$ on the dense vectors.\n",
    "\n",
    "To restrict the search space over potential functions, the authors impose a number of constraints they think $F$ should satisfy:\n",
    "1. > While $F$ could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture. $F$ should be dot-product based.\n",
    "2. > The distinction between a word and a context word is arbitrary and we are free to exchange the two roles. To do so consistently, we must not only exchange $w \\leftrightarrow \\tilde{w}$ but also $X \\leftrightarrow X^T$.\n",
    "3. > It should be well-defined for all values in $X$.\n",
    "\n",
    "Given these three constraints, each word $i$ in our vocabulary is represented by four vectors:\n",
    "1. A vector $w_i \\in \\mathbb{R}^D$\n",
    "2. A bias $b_i \\in \\mathbb{R}$\n",
    "3. A context vector $\\tilde{w}_i \\in \\mathbb{R}^D$\n",
    "4. A context bias $\\tilde{b}_i \\in \\mathbb{R}$\n",
    "\n",
    "and $F: \\mathbb{R}^D \\times \\mathbb{R} \\times \\mathbb{R}^D \\times \\mathbb{R} \\to \\mathbb{R}$ is defined as:\n",
    "\n",
    "$F(w_i, \\tilde{w}_k, b_i, \\tilde{b}_k) = w_i^T\\tilde{w}_k + b_i + \\tilde{b}_k$.\n",
    "\n",
    "Or equivalently the least squares error $J$ is minimized, where:\n",
    "\n",
    "$J = \\sum_{i,j=1}^{V} f(X_{ij})(w_{i}^T\\tilde{w}_j + b_i + \\tilde{b}_j - log(X_{ij}))^2$\n",
    "\n",
    "with $f$ being a weighting function, defined as \n",
    "\n",
    "$f: \\mathbb{R} \\to \\mathbb{R} = \\begin{cases}\n",
    "    (x/x_{max})^\\alpha, & \\text{if $x<x_{max}$}\\\\\n",
    "    1, & \\text{otherwise}.\n",
    "  \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting Function\n",
    "\n",
    "Let's start with the last part. \n",
    "\n",
    "**Coding 4**: Complete the weighting function `weight_fn` which accepts a co-occurrence matrix ${X}$, a maximum value $x_{max}$ and a fractional power $alpha$, and returns the weighted co-occurrence matrix $f({X})$.\n",
    "\n",
    "Then, compute $\\text{X_weighted}$, the matrix ${X}$ after weighting, using the paper's suggested parameters. \n",
    "\n",
    "_Hint_: Note that $f$ is defined pointwise, so our weighting function should also be pointwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_fn(X: FloatTensor, x_max: int, a: float) -> FloatTensor:\n",
    "    X_adj = (X/x_max)**a\n",
    "    X_ones = torch.ones_like(X)\n",
    "    X_weighted = torch.where(X < x_max, X_adj, X_ones)\n",
    "    return X_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056],\n",
      "        [0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056],\n",
      "        [0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056],\n",
      "        ...,\n",
      "        [0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056],\n",
      "        [0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056],\n",
      "        [0.0056, 0.0056, 0.0056,  ..., 0.0056, 0.0056, 0.0056]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DK\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_weighted = torch.tensor(weight_fn(X, x_max=100, a=0.75)).to(torch.float)\n",
    "print(X_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get an understanding of how the weighting affects different co-occurrence values (high and low). Think of some word pairs with high and low co-occurrence and look them up in $X$ and in $\\text{X_weighted}$ to get a better idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"tea\", \"wand\", \"spell\", probe(\"tea\", \"wand\", \"spell\", vocab, P))\\nprint(\"tea\", \"wand\", \"cup\", probe(\"tea\", \"wand\", \"cup\", vocab, P))\\nprint()\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(\"tea\", \"wand\", \"spell\", probe(\"tea\", \"wand\", \"spell\", vocab, P))\n",
    "print(\"tea\", \"wand\", \"cup\", probe(\"tea\", \"wand\", \"cup\", vocab, P))\n",
    "print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Show the completed code to your teacher before proceeding</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "Next step is to write the loss function. \n",
    "\n",
    "We can write it as a pointwise function, apply it iteratively over each pair of words and then sum the result; that's however extremely inefficient. \n",
    "\n",
    "Inspecting the formulation of $J$, it is fairly straight-forward to see that it can be immediately implemented using matrix-matrix operations, as:\n",
    "\n",
    "$J = \\sum_{i,j=1}^{V}f(\\mathbf{X})\\cdot(W\\tilde{W}^T + b + \\tilde{b}^T - log(X))^2$,\n",
    "\n",
    "where $W$, $\\tilde{W}$ are the $N \\times D$ matrices containing the $D$-dimensional vectors of all our $N$ vocabulary words, and $b$, $\\tilde{b}$ are the $N \\times 1$ matrices containing the $1$-dimensional biases of our words.\n",
    "\n",
    "**Coding 5**: Complete `loss_fn`, a function that accepts a weighted co-occurrence matrix $f({X})$, the word vectors and biases $W$, $\\tilde{W}$, $b$, $\\tilde{b}$ and the co-occurrence matrix ${X}$, and computes $J$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    X_weighted: FloatTensor, \n",
    "    W: FloatTensor, \n",
    "    W_context: FloatTensor, \n",
    "    B: FloatTensor, \n",
    "    B_context: FloatTensor, \n",
    "    X: FloatTensor\n",
    ") -> FloatTensor:\n",
    "    \n",
    "    WW = torch.mm(W, W_context.T)\n",
    "    LX = torch.log(X)\n",
    "    return (X_weighted *(WW + B + B_context.T - LX)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Show the completed code to your teacher before proceeding</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "\n",
    "We have the normalized co-occurrence matrix ${X}$, the weighting function $f$, and the loss function $J$ that implements $F$.\n",
    "\n",
    "What we need now is a mapping from words (or word ids) to unique, parametric and trainable vectors. \n",
    "\n",
    "Torch provides this abstraction in the form of [Embedding layers](https://pytorch.org/docs/stable/nn.html#embedding). Each such layer may be viewed as a stand-alone network that can be optimized using the standard procedure we have already seen. \n",
    "\n",
    "We will utilize the `nn.Module` class to contain all our embedding layers and streamline their joint optimization.\n",
    "The container class will be responsible for a few things:\n",
    "\n",
    "1. **Coding 6.1**: Wrapping the embedding layers:\n",
    "    1. A vector embedding that maps words to $w \\in \\mathbb{R}^D$\n",
    "    2. A context vector embedding that maps words to $w_c \\in \\mathbb{R}^D$\n",
    "    3. A bias embedding that maps words to $b \\in \\mathbb{R}^1$\n",
    "    4. A context bias embedding that maps words to $b_c \\in \\mathbb{R}^1$\n",
    "2. **Coding 6.2**: Implementing `forward`, a function that accepts a weighted co-occurrence matrix $f(X)$, the co-occurrence matrix $X$, then finds the embeddings of all words and finally calls `loss_fn` as defined above.\n",
    "3. **Coding 7**: Implementing `get_vectors`, a function that receives no input and produces the word vectors and context word vectors of all words, adds them together and returns the result, in accordance with the paper:\n",
    "> ...With this in mind, we choose to use the sum $W + \\tilde{W}$ as our word vectors.\n",
    "\n",
    "Complete the network class following the above specifications.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(torch.nn.Module):\n",
    "    def __init__(self, vocab: Dict[str, int], vector_dimensionality: int=30, device: str='cpu') -> None:\n",
    "        super(GloVe, self).__init__()\n",
    "        self.device = device\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.w = nn.Embedding(self.vocab_len, vector_dimensionality).to(device)\n",
    "        self.wc = nn.Embedding(self.vocab_len, vector_dimensionality).to(device)\n",
    "        self.b = nn.Embedding(self.vocab_len, 1).to(device)\n",
    "        self.bc = nn.Embedding(self.vocab_len, 1).to(device)\n",
    "        \n",
    "    def forward(self, X_weighted: FloatTensor, X: FloatTensor) -> FloatTensor:\n",
    "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
    "        W = self.w(embedding_input)\n",
    "        WC = self.wc(embedding_input)\n",
    "        B = self.b(embedding_input)\n",
    "        BC = self.bc(embedding_input)\n",
    "        \n",
    "        J = loss_fn(X_weighted, W, WC, B, BC, X)\n",
    "        return J\n",
    "    \n",
    "    def get_vectors(self) -> FloatTensor:\n",
    "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
    "        W = self.w(embedding_input)\n",
    "        WC = self.wc(embedding_input)\n",
    "        W_WC = W + WC\n",
    "        return W_WC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Show the completed code to your teacher before proceeding</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Everything is in place; now we may begin optimizing our embedding layers (and in doing so, the vectors they assign). \n",
    "\n",
    "**Coding 8.1**: Instantiate the network class you just defined using $D = 30$. Then instantiate an `Adam` optimizer with a learning rate of 0.05 and train your network for 300 epochs.\n",
    "\n",
    "When writing the training script, remember that your network's forward pass is __already__ computing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = GloVe(vocab, 30)\n",
    "opt = optim.Adam(network.parameters(), lr = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss: 11891599.0\n",
      "Iteration 10 loss: 3183803.25\n",
      "Iteration 20 loss: 1180905.625\n",
      "Iteration 30 loss: 571355.25\n",
      "Iteration 40 loss: 412136.6875\n",
      "Iteration 50 loss: 356657.5625\n",
      "Iteration 60 loss: 332617.1875\n",
      "Iteration 70 loss: 320195.625\n",
      "Iteration 80 loss: 311718.0625\n",
      "Iteration 90 loss: 304695.34375\n",
      "Iteration 100 loss: 298673.0625\n",
      "Iteration 110 loss: 293585.5625\n",
      "Iteration 120 loss: 289280.25\n",
      "Iteration 130 loss: 285644.53125\n",
      "Iteration 140 loss: 282590.3125\n",
      "Iteration 150 loss: 280019.65625\n",
      "Iteration 160 loss: 277837.75\n",
      "Iteration 170 loss: 275964.71875\n",
      "Iteration 180 loss: 274340.40625\n",
      "Iteration 190 loss: 272916.3125\n",
      "Iteration 200 loss: 271655.90625\n",
      "Iteration 210 loss: 270527.78125\n",
      "Iteration 220 loss: 269506.875\n",
      "Iteration 230 loss: 268573.59375\n",
      "Iteration 240 loss: 267713.96875\n",
      "Iteration 250 loss: 266916.90625\n",
      "Iteration 260 loss: 266174.25\n",
      "Iteration 270 loss: 265479.8125\n",
      "Iteration 280 loss: 264828.375\n",
      "Iteration 290 loss: 264215.125\n",
      "Iteration 300 loss: 263637.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 301\n",
    "losses = []\n",
    "for i in range(num_epochs):\n",
    "    loss = network.forward(X_weighted, X) # loss computation \n",
    "    loss.backward() # gradient computation\n",
    "    opt.step() # back-propagation\n",
    "    opt.zero_grad() # gradient reset\n",
    "    losses.append(loss)\n",
    "    if i%10 == 0:\n",
    "        print('Iteration {} loss: {}'.format(i,  loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding 8.2**: Plot the losses and examine the learning curve. Is its shape what you would expect it to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,num_epochs,1), np.log(losses)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Show the completed code to your teacher before proceeding</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation (Similarity)\n",
    "\n",
    "Curious to see what this network has learned? Let's perform a simple validation experiment. \n",
    "\n",
    "We will check which words the models considers the most similar to other words. To that end, we need a notion of __similarity__. One of the most common measures of similarity in high dimensional vector spaces is the cosine similarity. \n",
    "\n",
    "The cosine similarity of two vectors $\\vec{a}, \\vec{b}$ is given as:\n",
    "$$sim(\\vec{a}, \\vec{b}) = \\frac{\\vec{a}\\cdot \\vec{b}}{|\\vec{a}|_2 \\cdot |\\vec{b}|_2}$$\n",
    "\n",
    "where $|\\vec{x}|_2$ is the $L_2$-norm of the $\\vec{x}$.\n",
    "\n",
    "The function `similarity` below accepts two words, a vocabulary and the network's output vectors, and computes the similarity between these two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word_i: str, word_j: str, vocab: Dict[str, int], vectors: FloatTensor) -> float:\n",
    "    i = vocab[word_i]\n",
    "    j = vocab[word_j] \n",
    "    v_i = vectors[i] / torch.norm(vectors[i], p=2)  # a/|a|\n",
    "    v_j = vectors[j] / torch.norm(vectors[j], p=2)  # b/|b|\n",
    "    sim = torch.mm(v_i.view(1, -1), v_j.view(-1, 1)).item()\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out some examples. Consider the word pairs below and, optionally, add your own word pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'cruciatus' and 'imperius' is: 0.6526349782943726\n",
      "Similarity between 'avada' and 'kedavra' is: 0.8372835516929626\n",
      "Similarity between 'hogwarts' and 'school' is: 0.5365726947784424\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.006154807284474373\n",
      "Similarity between 'giant' and 'hagrid' is: 0.6696826815605164\n",
      "Similarity between 'avada' and 'spell' is: 0.5213215947151184\n",
      "Similarity between 'kedavra' and 'spell' is: 0.42069360613822937\n",
      "Similarity between 'tea' and 'spell' is: -0.18944574892520905\n",
      "Similarity between 'flavor' and 'spell' is: -0.2212987095117569\n"
     ]
    }
   ],
   "source": [
    "word_vectors = network.get_vectors().detach()\n",
    "\n",
    "for pair in [\n",
    "    (\"cruciatus\", \"imperius\"), \n",
    "    (\"avada\", \"kedavra\"), \n",
    "    (\"hogwarts\", \"school\"), \n",
    "    (\"goblin\", \"hagrid\"), \n",
    "    (\"giant\", \"hagrid\"),\n",
    "    (\"avada\", \"spell\"),\n",
    "    (\"kedavra\", \"spell\"),\n",
    "    (\"tea\", \"spell\"),\n",
    "    (\"flavor\", \"spell\"),\n",
    "]:\n",
    "    \n",
    "    print(\"Similarity between '{}' and '{}' is: {}\".\n",
    "          format(pair[0], pair[1], similarity(pair[0], pair[1], vocab, word_vectors)))\n",
    "    \n",
    "# Similarity between 'cruciatus' and 'imperius' is: 0.6526349782943726\n",
    "# Similarity between 'avada' and 'kedavra' is: 0.8372835516929626\n",
    "# Similarity between 'hogwarts' and 'school' is: 0.5365726947784424\n",
    "# Similarity between 'goblin' and 'hagrid' is: 0.006154807284474373\n",
    "# Similarity between 'giant' and 'hagrid' is: 0.6696826815605164\n",
    "# Similarity between 'avada' and 'spell' is: 0.5213215947151184\n",
    "# Similarity between 'kedavra' and 'spell' is: 0.42069360613822937\n",
    "# Similarity between 'tea' and 'spell' is: -0.18944574892520905\n",
    "# Similarity between 'flavor' and 'spell' is: -0.2212987095117569"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation 3**: Give a brief interpretation of the results. Do the scores correspond well to your perceived similarity of these word pairs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3:** The results are reasonable, both avada and kedavra are expected to be very similar, since they are both part of a spell and therefore almost always co-occur in succession. Similarly, hogwarts and school should be relatively similar in this corpus, since hogwarts is one school of wizards and is almost always followed by the word school. Also, hagrid and giant are similar, because hagrid is large, not small like a goblin and also does not look like one.\n",
    "Unlike \"avada\" and \"kedavra\", that are similar to the word 'spell' (since both are part of a spell), \"tea\" and \"spell\" or \"flavor\" and \"spell\" are not conceptually (or semantically) related to spell. However they are also not conceptually inversed, thus dissimilar (the complete opposite, such as antonyms). Hence their value should be close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the similarities of one word against all other words in the corpus, we may rewrite the above equation as:\n",
    "$$sim(\\vec{w}, \\mathbf{C}) = \\frac{\\vec{w}\\cdot \\mathbf{C}}{|\\vec{w}|_2 \\cdot |\\mathbf{C}|_2}$$\n",
    "\n",
    "**Coding 9**: Using `similarity` as a reference, write `similarities`, which accepts one word, a vocabulary and the network's output vectors and computes the similarity between the word and the entire corpus.\n",
    "\n",
    "_Hint_: $\\mathbf{C} \\in \\mathbb{R}^{N, D}$, $\\vec{w} \\in \\mathbb{R}^{1, D}$, $sim(\\vec{w}, \\mathbf{C}) \\in \\mathbb{R}^{1, N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarities(word_i: str, vocab: Dict[str, int], vectors: FloatTensor) -> FloatTensor:\n",
    "    i = vocab[word_i]\n",
    "    v_w = vectors[i] / torch.norm(vectors[i], p=2)  # w/|w|\n",
    "    v_C = vectors / torch.norm(vectors, p=2)  # C/|C|\n",
    "    sim = torch.mm(v_w.view(1, -1), v_C.permute(1, 0))\n",
    "    return sim  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'hagrid' and the corpus is: tensor([[ 0.0005,  0.0063,  0.0019,  ..., -0.0020, -0.0025,  0.0030]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity between '{}' and the corpus is: {}\".format(\"hagrid\", similarities(\"hagrid\", vocab, word_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Show the completed code to your teacher before proceeding</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can manipulate the word vectors to find out what the corpus-wide most similar words to a query word are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word_i: str, vocab: Dict[str, int], vectors: FloatTensor, k: int) -> List[str]:\n",
    "    sims = similarities(word_i, vocab, vectors)\n",
    "    _, topi = sims.topk(dim=-1, k=k)\n",
    "    topi = topi.view(-1).cpu().numpy().tolist()\n",
    "    inv = {v: i for i, v in vocab.items()}\n",
    "    return [inv[i] for i in topi if inv[i] != word_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'forbidden': ['forest', 'buckbeak', 'madame', 'students', 'spiders', 'maxime']\n",
      "Most similar words to 'myrtle': ['moaning', 'bathroom', 'toilet', 'tank', 'sorting', 'water']\n",
      "Most similar words to 'gryffindor': ['slytherin', 'team', 'ravenclaw', 'bludger', 'quidditch', 'points']\n",
      "Most similar words to 'wand': ['harry', 't', 'eyes', 'ron', 'just', 'said']\n",
      "Most similar words to 'quidditch': ['team', 'gryffindor', 'match', 'snitch', 'wood', 'got']\n",
      "Most similar words to 'marauder': ['map', 'nigellus', 'published', 'padfoot', 'pomfrey', 'portrait']\n",
      "Most similar words to 'horcrux': ['soul', 'godric', 'sword', 'bathilda', 'hallows', 'did']\n",
      "Most similar words to 'phoenix': ['feather', 'fawkes', 'wand', 'gaunt', 'ollivander', 'eyes']\n",
      "Most similar words to 'triwizard': ['tournament', 'champions', 'cup', 'beauxbatons', 'madame', 'bagman']\n",
      "Most similar words to 'screaming': ['light', 'supporters', 'death', 'expecto', 'eater', 'himself']\n",
      "Most similar words to 'letter': ['envelope', 'hedwig', 'owls', 'owl', 'ripped', 'quill']\n"
     ]
    }
   ],
   "source": [
    "for word in [\n",
    "    \"forbidden\", \"myrtle\", \"gryffindor\", \"wand\", \"quidditch\", \"marauder\",\n",
    "    \"horcrux\", \"phoenix\", \"triwizard\", \"screaming\", \"letter\"\n",
    "]:\n",
    "    print(\"Most similar words to '{}': {}\".format(word, most_similar(word, vocab, word_vectors, 7)))\n",
    "    \n",
    "# Results:\n",
    "# Most similar words to 'forbidden': ['forest', 'buckbeak', 'madame', 'students', 'spiders', 'maxime']\n",
    "# Most similar words to 'myrtle': ['moaning', 'bathroom', 'toilet', 'tank', 'sorting', 'water']\n",
    "# Most similar words to 'gryffindor': ['slytherin', 'team', 'ravenclaw', 'bludger', 'quidditch', 'points']\n",
    "# Most similar words to 'wand': ['harry', 't', 'eyes', 'ron', 'just', 'said']\n",
    "# Most similar words to 'quidditch': ['team', 'gryffindor', 'match', 'snitch', 'wood', 'got']\n",
    "# Most similar words to 'marauder': ['map', 'nigellus', 'published', 'padfoot', 'pomfrey', 'portrait']\n",
    "# Most similar words to 'horcrux': ['soul', 'godric', 'sword', 'bathilda', 'hallows', 'did']\n",
    "# Most similar words to 'phoenix': ['feather', 'fawkes', 'wand', 'gaunt', 'ollivander', 'eyes']\n",
    "# Most similar words to 'triwizard': ['tournament', 'champions', 'cup', 'beauxbatons', 'madame', 'bagman']\n",
    "# Most similar words to 'screaming': ['light', 'supporters', 'death', 'expecto', 'eater', 'himself']\n",
    "# Most similar words to 'letter': ['envelope', 'hedwig', 'owls', 'owl', 'ripped', 'quill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation 4**: Interpret the results.\n",
    "- Do these most similar words make sense (are they actually similar to the query words)? \n",
    "- Are there any patterns you can see in the \"errors\" (the words that you woudn't consider actually similar to the query word)? \n",
    "- Would you say that the model captures similarity, relatedness, both or neither?\n",
    "- Any other observations are welcome.\n",
    "\n",
    "Illustrate your answers with examples from your model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4:** \n",
    "**(1)** On face value, some categories have extreme good matches, such as the results for: \n",
    "* (1) letter ['envelope', 'hedwig', 'owls', 'owl', 'ripped', 'quill'], since both quill and evelope are generally related to the concept of letter, and hedweig and owls are specifically related to it in the context of the HP books\n",
    "* (2) Gryffindor: ['slytherin', 'team', 'ravenclaw', 'bludger', 'quidditch', 'points'], since the other houses (slythernin, ravenclaw) are fairy similiar conceptually, while aspects of quidditch (Quidditch, points, team) are more strongly related to Gryffindor, because they often engage in this activity with the other houses.\n",
    "* (3) myrtle': ['moaning', 'bathroom', 'toilet', 'tank', 'sorting', 'water'], where both moaning and bathroom related concepts (such as toilet and water) do often coocur in the context of myrte, but are rather related than similiar to her. \n",
    "\n",
    "The similarity/relatedness for other concepts such as marauder, screaming, and forbidden are more difficult to evaluate. This might be because HP-specific concepts such as Griffindor or Myrte have more HP-specific related words (Quidditch, Bathroom) rather than more general words (like in the case of wand). Some words might actually be related but this is hard to evaluate for HP novices and should rather be done by a true expert in this field, like Thimo's girlfriend.\n",
    "\n",
    "**(2)** Some errors seem to occur, but it could also be that those words are related to the target word, but due to the lack of HP-knowledge we might not see their relationship. However,  words such as conjugations or often occuring verbs might co-occur a lot with a given concept but are conceptually neither related nor similar to it. Other words that are not in particularly related to a concept such as published and marauder might jut co-occur often enough for the model to assume that there might be a semantic relationship between them. In general the most similar words are strongly specific to the HP-corpus (e.g. Owl and letter might not necessarly be related to each other outside of the corpus). Lastly, since the book is strongly written from Harries percpetive (although its in a narrator view), Harry is often related to many concepts (due to the writing style favoring Harries perspective more than others).\n",
    "\n",
    "**(3)** As mentioned before, many word pairs are related to each other rather than similar. Similarity is different from the general notion of relatedness:\n",
    "* (1) Gryffindor - Quidditch are related (occur in the same contexts, more general relationship)\n",
    "* (2) Gryffindor - Slytherin are similar (substitutable in many contexts, share properties)\n",
    "\n",
    "In general, similarity is much harder to extract out of distributional spaces. Typically, related words occur in the same context as well as similar words. Again, the GloVe model is based on the paradigm of distributed semantics, and similarity in this context is a very broad notion. Relatedness will also be included in this paradigmn. Although some notion of similarity could be captured and possibly more whith additional processing, such as retrofitting, are introduced to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall it's quite impressive; we managed to encode a meaningful portion of the corpus statistics in only $30$ numbers per word! \n",
    "(A compression ratio of 99.4%)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The word vectors obtained by this process are (to a small extent) random, due to the random initialization of the embedding layers. If you are unhappy with your results, you can repeat the experiment a few times or try to toy around with the hyper-parameters (the smoothing factor of ${X}$, $x_{max}$, $\\alpha$, the number of epochs and the dimensionality of the vector space).\n",
    "</div>\n",
    "\n",
    "Word vectors, however, contain way more information than just word co-occurrence statistics. Hold tight until the next assignment, where we will see how word vectors may be used to infer information spanning entire phrases and sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation (Word Analogies)\n",
    "\n",
    "From the paper:\n",
    "> The word analogy task consists of questions like \"$a$ is to $b$ as is $c$ to $?$\" To correctly answer this question, we must find the word $d$ such that $w_d \\approx w_b - w_a + w_c$ according to the cosine similarity.\n",
    "\n",
    "**Coding 10**: Write your own function that performs the word analogy task.\n",
    "\n",
    "_Hint_: Take a look at the code a few cells back. Most of what you need is already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(\n",
    "    word_a: str, word_b: str, word_c: str, vocab: Dict[str, int], vectors: FloatTensor, k: int\n",
    ") -> List[str]:\n",
    "    # Word vectors\n",
    "    w_a = vectors[vocab[word_a]]\n",
    "    w_b = vectors[vocab[word_b]]\n",
    "    w_c = vectors[vocab[word_c]]\n",
    "    w_d = w_b - w_a + w_c\n",
    "\n",
    "    # Same as similarities\n",
    "    w = w_d.unsqueeze(0) / torch.norm(w_d, p=2)\n",
    "    C_m = vectors.T\n",
    "    C = C_m / torch.norm(C_m, p=2, dim=0)\n",
    "    sims = torch.mm(w, C)\n",
    "\n",
    "    # Same as most similar\n",
    "    _, topi = sims.topk(dim=-1, k=k)\n",
    "    topi = topi.view(-1).cpu().numpy().tolist()\n",
    "    inv = {v: i for i, v in vocab.items()}\n",
    "    return [inv[i] for i in topi if inv[i] != word_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Show the completed code to your teacher before proceeding</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example triplets to test your analogies on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'padma' is to 'parvati' as 'fred' is to ['aren', 'hang', 'bit', 'actually', 'started']\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'stag', 'dementor', 'vapor', 'sparks']\n",
      "'dungeon' is to 'slytherin' as 'tower' is to ['slytherin', 'heir', 'championship', 'opposite', 'played']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['dear', 'ginny', 'horn', 'landed', 'heavily']\n",
      "'ron' is to 'molly' as 'draco' is to ['narcissa', 'molly', 'runcorn', 'yaxley', 'nott', 'borgin']\n",
      "'durmstrang' is to 'viktor' as 'beauxbatons' is to ['viktor', 'krum', 'locker', 'champions', 'applauding']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['divination', 'classroom', 'sprout', 'greenhouse', 'term']\n",
      "'harry' is to 'seeker' as 'ron' is to ['seeker', 'chess', 'captain', 'teams', 'played', 'oliver']\n"
     ]
    }
   ],
   "source": [
    "triplets = [(\"padma\", \"parvati\", \"fred\"),\n",
    "            (\"avada\", \"kedavra\", \"expecto\"),\n",
    "            (\"dungeon\", \"slytherin\", \"tower\"),\n",
    "            (\"scabbers\", \"ron\", \"hedwig\"),\n",
    "            (\"ron\", \"molly\", \"draco\"),\n",
    "            (\"durmstrang\", \"viktor\", \"beauxbatons\"),\n",
    "            (\"snape\", \"potions\", \"trelawney\"),\n",
    "            (\"harry\", \"seeker\", \"ron\")\n",
    "           ]\n",
    "\n",
    "for a, b, c in triplets:\n",
    "    print(\"'{}' is to '{}' as '{}' is to {}\".format(a, b, c, analogy(a, b, c, vocab, word_vectors, 6)))\n",
    "\n",
    "# Results:\n",
    "# 'padma' is to 'parvati' as 'fred' is to ['aren', 'hang', 'bit', 'actually', 'started']\n",
    "# 'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'stag', 'dementor', 'vapor', 'sparks']\n",
    "# 'dungeon' is to 'slytherin' as 'tower' is to ['slytherin', 'heir', 'championship', 'opposite', 'played']\n",
    "# 'scabbers' is to 'ron' as 'hedwig' is to ['dear', 'ginny', 'horn', 'landed', 'heavily']\n",
    "# 'ron' is to 'molly' as 'draco' is to ['narcissa', 'molly', 'runcorn', 'yaxley', 'nott', 'borgin']\n",
    "# 'durmstrang' is to 'viktor' as 'beauxbatons' is to ['viktor', 'krum', 'locker', 'champions', 'applauding']\n",
    "# 'snape' is to 'potions' as 'trelawney' is to ['divination', 'classroom', 'sprout', 'greenhouse', 'term']\n",
    "# 'harry' is to 'seeker' as 'ron' is to ['seeker', 'chess', 'captain', 'teams', 'played', 'oliver']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some minimal emergent intelligence :) *(hopefully..)*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation 5**: Interpret the results. \n",
    "- Did the model manage to guess the correct answers to the analogies (taking the first word in the output to be the model's \"guess\")? \n",
    "- Are the correct answers present in the top K words? \n",
    "- Do you see any patterns in the cases when the model didn't solve the task correctly? In other words, when the model's guess was wrong, can you suggest why the model guessed what it guessed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5** \n",
    "**(1)** The model is able to describe some analogies correctly: For example, \n",
    "* 'avada' is to 'kedavra' as 'expecto' is to 'patronum'. Both are aspects of spells.\n",
    "* 'snape' is to 'potion' as 'trelaweny' is to 'divination'. Both are professors that teach within a certain field of magic.\n",
    "* 'ron' is to 'molly' as 'draco' is to 'narcissa'. Both molly and narcissa are the mothers of ron and draco, respectively.\n",
    "* 'harry' is to 'seeker' as 'ron' is to 'chess'. This is debateable, since the chess is not the one that the models considers as best match. Morover, in the context of quidditch ron would rather be the keeper (together with oliver), but in the context of skilledness or passion chess would be reasonable for ron. \n",
    "\n",
    "Yet following are wrong:\n",
    "* 'dungeon' is to 'slytherin' and 'tower' is to... (is wrong), should be Griffindor (or another house).\n",
    "* 'padma' is to 'parvati' as 'fred' is to... (is wrong, should be Ron, George, Bill (and two unknown) would be correct.\n",
    "* 'durmstrang' is to 'viktor' as 'beauxbatons' is to ... (is wrong), should be Fleur.\n",
    "* 'scabbers' is to 'ron' as 'hedwig' is to ... (is wrong), should be Harry.\n",
    "\n",
    "**(2)** The four examples mentioned above has the correct answers present in the top 6 words, of which 3 are even considered as the best match. However none of the other examples contains the correct answer in the vector of words.\n",
    "\n",
    "**(3)**  Asaformetioned, there are some factor that might hamper the models performance to understand similarity and relatedness between words. Those factors affect how word vectors are constructed and therefore those problems are passed on to the analysis of analogies. Three factors were mentioned in answer 3, namely, common coocurances, grammatical aspects and stopwords and a writing style favoring Harry's perspective (there are probably more challanges for this corpus or distributed semantics in general).\n",
    "\n",
    "One error in the case of 'scabbers' is to 'ron' as 'hedwig' is to ... might give a cue why incorrect answers occur. Dear and Ginny are the models best guess, and they are strongly related to Hedwig and Harry, since Harry might often receive or send letter to Ginny via Hedwig. The same goes for 'harry' is to 'seeker' as 'ron' is to ... several very related words to ron/harry and quidditch occur that are even somewhat similar to rons role in quidditch, such as oliver (also keeper) and captain (another position/role of Harry), respectively. \n",
    "\n",
    "One common pattern often seems to arise from the analogies for specific word pairs, where sometimes the model returns the second pair of the first part of the analogy as the correct answer (like, $a$ is to $b$, as $c$ is to $b$). This is in particular the case when the loss is high during the first few epochs, some concepts \"'avada' is to 'kedavra' as 'expecto' is to 'patronum'\"are fairly fast figured out while others keep showing the errorlike pattern, indicating that the model did not unstand the semantic relationship between the words. The model might not arrive at the right word vector for $a$ and $b$, $w_d \\approx w_b - w_a + w_c$, where $w_a + w_c$ are 0 and hence $w_d =w_b$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional\n",
    "If you are done, you can continue experimenting in order to understand the system's behaviour better. For example: how does training and hyperparameter choice affect the model's performance?\n",
    "Repeat the training using your own hyperparameters (vector space dimensionality, optimizer parameters, the number of training epochs, etc.). \n",
    "\n",
    "During the training loop, print the qualitative benchmarks every few epochs. Do they keep improving? Is there any disadvantage to exhaustively training until convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DK\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration 0 loss: 15530515.0\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: -0.11525538563728333\n",
      "Similarity between 'avada' and 'kedavra' is: 0.0590939000248909\n",
      "Similarity between 'hogwarts' and 'school' is: -0.07880018651485443\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.062449563294649124\n",
      "Similarity between 'giant' and 'hagrid' is: -0.032696884125471115\n",
      "Similarity between 'tea' and 'spell' is: 0.1123008131980896\n",
      "Similarity between 'flavor' and 'spell' is: -0.016587352380156517\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['kedavra', 'mouse', 'vernon', 'gain', 'dressing']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['ron', 'mane', 'paid', 'lingered', 'tent']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'phineas', 'veritaserum', 'mopping', 'harsh']\n",
      "'harry' is to 'seeker' as 'ron' is to ['seeker', 'telephone', 'hunched', 'dripping', 'fangs']\n",
      "\n",
      " Iteration 20 loss: 356207.125\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.15682575106620789\n",
      "Similarity between 'avada' and 'kedavra' is: 0.43603000044822693\n",
      "Similarity between 'hogwarts' and 'school' is: 0.1759805977344513\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.23882436752319336\n",
      "Similarity between 'giant' and 'hagrid' is: 0.37577101588249207\n",
      "Similarity between 'tea' and 'spell' is: 0.10405291616916656\n",
      "Similarity between 'flavor' and 'spell' is: 0.12553726136684418\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['kedavra', 'chimney', 'maladies', 'trembled', 'player']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['ron', 'said', 'like', 'help', 't']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'smaller', 'hesitated', 'lights', 'classroom']\n",
      "'harry' is to 'seeker' as 'ron' is to ['warrington', 'seeker', 'ripping', 'offense', 'attic']\n",
      "\n",
      " Iteration 40 loss: 201967.8125\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.22963285446166992\n",
      "Similarity between 'avada' and 'kedavra' is: 0.5206027626991272\n",
      "Similarity between 'hogwarts' and 'school' is: 0.44817495346069336\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.10158053040504456\n",
      "Similarity between 'giant' and 'hagrid' is: 0.33754104375839233\n",
      "Similarity between 'tea' and 'spell' is: 0.13340874016284943\n",
      "Similarity between 'flavor' and 'spell' is: 0.24218153953552246\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'knocked', 'knelt', 'bothered', 'rushing']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['ron', 'said', 'just', 't', 'took']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'ice', 'doing', 'products', 'growled']\n",
      "'harry' is to 'seeker' as 'ron' is to ['warrington', 'offense', 'guide', 'season', 'nights', 'flush']\n",
      "\n",
      " Iteration 60 loss: 166186.875\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.42312291264533997\n",
      "Similarity between 'avada' and 'kedavra' is: 0.7014347910881042\n",
      "Similarity between 'hogwarts' and 'school' is: 0.5593755841255188\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.12070540338754654\n",
      "Similarity between 'giant' and 'hagrid' is: 0.4649333953857422\n",
      "Similarity between 'tea' and 'spell' is: 0.04921824485063553\n",
      "Similarity between 'flavor' and 'spell' is: 0.15211382508277893\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'stag', 'screaming', 'rushing', 'fog']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['ron', 'said', 'took', 'just', 'like']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'chart', 'clipboard', 'thursday', 'real']\n",
      "'harry' is to 'seeker' as 'ron' is to ['season', 'seeker', 'warrington', 'played', 'honest', 'bracingly']\n",
      "\n",
      " Iteration 80 loss: 153216.625\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5195648074150085\n",
      "Similarity between 'avada' and 'kedavra' is: 0.7340966463088989\n",
      "Similarity between 'hogwarts' and 'school' is: 0.598163902759552\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.12141422182321548\n",
      "Similarity between 'giant' and 'hagrid' is: 0.521305501461029\n",
      "Similarity between 'tea' and 'spell' is: -0.08411090075969696\n",
      "Similarity between 'flavor' and 'spell' is: 0.09024873375892639\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'stag', 'fog', 'screaming', 'rushing']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['ron', 'said', 'just', 'hermione', 'took']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'chart', 'thursday', 'clipboard', 'results']\n",
      "'harry' is to 'seeker' as 'ron' is to ['seeker', 'played', 'season', 'bracingly', 'honest', 'goalposts']\n",
      "\n",
      " Iteration 100 loss: 146685.265625\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5257663726806641\n",
      "Similarity between 'avada' and 'kedavra' is: 0.758902907371521\n",
      "Similarity between 'hogwarts' and 'school' is: 0.6305093765258789\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.09936432540416718\n",
      "Similarity between 'giant' and 'hagrid' is: 0.5270292162895203\n",
      "Similarity between 'tea' and 'spell' is: -0.06318523734807968\n",
      "Similarity between 'flavor' and 'spell' is: 0.07837110757827759\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'fog', 'stag', 'screaming', 'lungs']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['ron', 'said', 'just', 'hermione', 'took']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'chart', 'thursday', 'patil', 'lesson']\n",
      "'harry' is to 'seeker' as 'ron' is to ['seeker', 'played', 'season', 'honest', 'bracingly', 'win']\n",
      "\n",
      " Iteration 120 loss: 142678.34375\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5222355723381042\n",
      "Similarity between 'avada' and 'kedavra' is: 0.7915067672729492\n",
      "Similarity between 'hogwarts' and 'school' is: 0.6467226147651672\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.0933755487203598\n",
      "Similarity between 'giant' and 'hagrid' is: 0.5181300640106201\n",
      "Similarity between 'tea' and 'spell' is: -0.03797352686524391\n",
      "Similarity between 'flavor' and 'spell' is: 0.08132205903530121\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'fog', 'stag', 'screaming', 'blinding']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['ron', 'just', 'said', 'hermione', 'took']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'chart', 'thursday', 'sherry', 'lesson']\n",
      "'harry' is to 'seeker' as 'ron' is to ['played', 'seeker', 'season', 'honest', 'bracingly', 'goalposts']\n",
      "\n",
      " Iteration 140 loss: 139932.34375\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5207263827323914\n",
      "Similarity between 'avada' and 'kedavra' is: 0.8204694986343384\n",
      "Similarity between 'hogwarts' and 'school' is: 0.6619793772697449\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.09476127475500107\n",
      "Similarity between 'giant' and 'hagrid' is: 0.5039409399032593\n",
      "Similarity between 'tea' and 'spell' is: -0.03545650094747543\n",
      "Similarity between 'flavor' and 'spell' is: 0.09679841995239258\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'fog', 'stag', 'screaming', 'blinding']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['ron', 'just', 'said', 'hermione', 'took']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'sherry', 'thursday', 'chart', 'schedules']\n",
      "'harry' is to 'seeker' as 'ron' is to ['played', 'seeker', 'goalposts', 'season', 'bracingly', 'honest']\n",
      "\n",
      " Iteration 160 loss: 137913.59375\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5193315744400024\n",
      "Similarity between 'avada' and 'kedavra' is: 0.8460527658462524\n",
      "Similarity between 'hogwarts' and 'school' is: 0.6739526391029358\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.09940998256206512\n",
      "Similarity between 'giant' and 'hagrid' is: 0.4881177246570587\n",
      "Similarity between 'tea' and 'spell' is: -0.0326533280313015\n",
      "Similarity between 'flavor' and 'spell' is: 0.11074280738830566\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'fog', 'stag', 'screaming', 'lungs']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['ron', 'just', 'said', 'hermione', 'took']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'sherry', 'schedules', 'thursday', 'binns']\n",
      "'harry' is to 'seeker' as 'ron' is to ['played', 'seeker', 'goalposts', 'bracingly', 'season', 'wood']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration 180 loss: 136354.671875\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5215564966201782\n",
      "Similarity between 'avada' and 'kedavra' is: 0.8667628169059753\n",
      "Similarity between 'hogwarts' and 'school' is: 0.6842138767242432\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.10296698659658432\n",
      "Similarity between 'giant' and 'hagrid' is: 0.4736907184123993\n",
      "Similarity between 'tea' and 'spell' is: -0.031296953558921814\n",
      "Similarity between 'flavor' and 'spell' is: 0.12075936794281006\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'fog', 'stag', 'screaming', 'lungs']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['ron', 'just', 'said', 'hermione', 't']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'sherry', 'schedules', 'thursday', 'binns']\n",
      "'harry' is to 'seeker' as 'ron' is to ['played', 'seeker', 'goalposts', 'bracingly', 'wood', 'dangling']\n",
      "\n",
      " Iteration 200 loss: 135106.875\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5273559093475342\n",
      "Similarity between 'avada' and 'kedavra' is: 0.8837835192680359\n",
      "Similarity between 'hogwarts' and 'school' is: 0.6931179761886597\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.10573737323284149\n",
      "Similarity between 'giant' and 'hagrid' is: 0.4605298638343811\n",
      "Similarity between 'tea' and 'spell' is: -0.030509963631629944\n",
      "Similarity between 'flavor' and 'spell' is: 0.126211017370224\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'fog', 'stag', 'screaming', 'wand:']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['ron', 'just', 'said', 'hermione', 't']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'schedules', 'sherry', 'thursday', 'divination']\n",
      "'harry' is to 'seeker' as 'ron' is to ['played', 'seeker', 'goalposts', 'bracingly', 'wood', 'thinking']\n",
      "\n",
      " Iteration 220 loss: 134080.1875\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5346630811691284\n",
      "Similarity between 'avada' and 'kedavra' is: 0.897875189781189\n",
      "Similarity between 'hogwarts' and 'school' is: 0.7004397511482239\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.10795039683580399\n",
      "Similarity between 'giant' and 'hagrid' is: 0.4482441544532776\n",
      "Similarity between 'tea' and 'spell' is: -0.02961704321205616\n",
      "Similarity between 'flavor' and 'spell' is: 0.12778186798095703\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'fog', 'stag', 'screaming', 'patronus']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['just', 'ron', 'said', 'hermione', 't']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'schedules', 'sherry', 'thursday', 'divination']\n",
      "'harry' is to 'seeker' as 'ron' is to ['played', 'seeker', 'goalposts', 'bracingly', 'thinking', 'wood']\n",
      "\n",
      " Iteration 240 loss: 133215.78125\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5420093536376953\n",
      "Similarity between 'avada' and 'kedavra' is: 0.9096881747245789\n",
      "Similarity between 'hogwarts' and 'school' is: 0.7061609625816345\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.10938774049282074\n",
      "Similarity between 'giant' and 'hagrid' is: 0.4363868534564972\n",
      "Similarity between 'tea' and 'spell' is: -0.028279049322009087\n",
      "Similarity between 'flavor' and 'spell' is: 0.1267230212688446\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'fog', 'stag', 'screaming', 'patronus']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['just', 'ron', 'said', 'hermione', 'like']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'schedules', 'sherry', 'divination', 'thursday']\n",
      "'harry' is to 'seeker' as 'ron' is to ['played', 'seeker', 'goalposts', 'thinking', 'bracingly', 'wood']\n",
      "\n",
      " Iteration 260 loss: 132474.75\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5485252737998962\n",
      "Similarity between 'avada' and 'kedavra' is: 0.919668972492218\n",
      "Similarity between 'hogwarts' and 'school' is: 0.7104883193969727\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.10997602343559265\n",
      "Similarity between 'giant' and 'hagrid' is: 0.42486515641212463\n",
      "Similarity between 'tea' and 'spell' is: -0.026542197912931442\n",
      "Similarity between 'flavor' and 'spell' is: 0.12421362102031708\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'fog', 'stag', 'patronus', 'screaming']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['just', 'ron', 'said', 'hermione', 'like']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'schedules', 'divination', 'sherry', 'thursday']\n",
      "'harry' is to 'seeker' as 'ron' is to ['seeker', 'played', 'thinking', 'goalposts', 'bracingly', 'wood']\n",
      "\n",
      " Iteration 280 loss: 131829.921875\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5537355542182922\n",
      "Similarity between 'avada' and 'kedavra' is: 0.928180992603302\n",
      "Similarity between 'hogwarts' and 'school' is: 0.7137081027030945\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.1098138615489006\n",
      "Similarity between 'giant' and 'hagrid' is: 0.4137325584888458\n",
      "Similarity between 'tea' and 'spell' is: -0.024519657716155052\n",
      "Similarity between 'flavor' and 'spell' is: 0.12119293212890625\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'stag', 'fog', 'patronus', 'drowning']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['just', 'ron', 'said', 'hermione', 'like']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'schedules', 'divination', 'results', 'sherry']\n",
      "'harry' is to 'seeker' as 'ron' is to ['seeker', 'played', 'thinking', 'wood', 'goalposts', 'bracingly']\n",
      "\n",
      " Iteration 300 loss: 131263.9375\n",
      "\n",
      " Similarities between word-pairs.\n",
      "Similarity between 'cruciatus' and 'imperius' is: 0.5574429035186768\n",
      "Similarity between 'avada' and 'kedavra' is: 0.9355342388153076\n",
      "Similarity between 'hogwarts' and 'school' is: 0.7160785794258118\n",
      "Similarity between 'goblin' and 'hagrid' is: 0.10910940915346146\n",
      "Similarity between 'giant' and 'hagrid' is: 0.4031709134578705\n",
      "Similarity between 'tea' and 'spell' is: -0.022266555577516556\n",
      "Similarity between 'flavor' and 'spell' is: 0.11827370524406433\n",
      "\n",
      " Analogies for the word tripplet.\n",
      "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'stag', 'fog', 'patronus', 'vapor']\n",
      "'scabbers' is to 'ron' as 'hedwig' is to ['just', 'ron', 'said', 'hermione', 'like']\n",
      "'snape' is to 'potions' as 'trelawney' is to ['potions', 'schedules', 'divination', 'results', 'sherry']\n",
      "'harry' is to 'seeker' as 'ron' is to ['seeker', 'played', 'thinking', 'wood', 'goalposts', 'bracingly']\n"
     ]
    }
   ],
   "source": [
    "network = GloVe(vocab, 70)\n",
    "opt = optim.Adam(network.parameters(), lr = 0.1)\n",
    "X_weighted = torch.tensor(weight_fn(X, x_max=100, a=0.85)).to(torch.float)\n",
    "num_epochs = 301\n",
    "losses = []\n",
    "\n",
    "# qualitative test instances for similarity\n",
    "word_pairs = [(\"cruciatus\", \"imperius\"), \n",
    "            (\"avada\", \"kedavra\"), \n",
    "            (\"hogwarts\", \"school\"), \n",
    "            (\"goblin\", \"hagrid\"), \n",
    "            (\"giant\", \"hagrid\"),\n",
    "            (\"tea\", \"spell\"),\n",
    "            (\"flavor\", \"spell\"),\n",
    "            ]\n",
    "# qualitative test instances for analogy\n",
    "triplets = [(\"avada\", \"kedavra\", \"expecto\"),\n",
    "            (\"scabbers\", \"ron\", \"hedwig\"),\n",
    "            (\"snape\", \"potions\", \"trelawney\"),\n",
    "            (\"harry\", \"seeker\", \"ron\")\n",
    "            ]\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss = network.forward(X_weighted, X) # loss computation \n",
    "    loss.backward() # gradient computation\n",
    "    opt.step() # back-propagation\n",
    "    opt.zero_grad() # gradient reset\n",
    "    losses.append(loss)\n",
    "    if i%20 == 0:\n",
    "        print(\"\\n\", 'Iteration {} loss: {}'.format(i,  loss.item()))\n",
    "        word_vectors = network.get_vectors().detach()\n",
    "        \n",
    "        print(\"\\n\", \"Similarities between word-pairs.\")\n",
    "        for pair in word_pairs:\n",
    "            print(\"Similarity between '{}' and '{}' is: {}\".\n",
    "            format(pair[0], pair[1], similarity(pair[0], pair[1], vocab, word_vectors)))\n",
    "        \n",
    "        print(\"\\n\", \"Analogies for the word tripplet.\")\n",
    "        for a, b, c in triplets:\n",
    "            print(\"'{}' is to '{}' as '{}' is to {}\".format(a, b, c, analogy(a, b, c, vocab, word_vectors, 6)))\n",
    "\n",
    "\n",
    "# with word_vector = 50 the initial loss (18M) is higher with the default setting (12M), however decreases faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** With a the following hyperparamters adjusted: word_vector = 70, learning_rate = 0.1, alpha = 0.85, the initial loss (15M) is somewhat higher than in the default setting (12M), persuambly due to the the larger word vector. The loss, however, decreases faster, than in the default setting reaching a minmum of 131K after 300 epochs. \n",
    "\n",
    "Qualitatively, the similarity steadyly increases with no abnormalities, reaching reasonable values at 300 epoches. For the analogies, only the obvious ones, such as 'avada' is to 'kedavra' as 'expecto' is to... resolve well very quickly, while the model is unable for most other tripplets to suggest the correct word as best fit, or to find a correct word at all. The analogies that could not be solved before, can also not be solved now and often $w_d = w_b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
